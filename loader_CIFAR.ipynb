{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from model import CNN\n",
    "from loss import loss_coteaching\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mixup import mixup_data\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from loader_CIFAR import CifarDataloader, CifarDataset\n",
    "\n",
    "\n",
    "# def unpickle(file):\n",
    "#     import _pickle as cPickle\n",
    "#     with open(file, 'rb') as fo:\n",
    "#         dict = cPickle.load(fo, encoding='latin1')\n",
    "#     return dict\n",
    "#\n",
    "#\n",
    "# class cifarDataset(Dataset):\n",
    "#     def __init__(self, dataset, root_dir, transform, mode, noise_file=''):\n",
    "#\n",
    "#         self.transform = transform\n",
    "#         self.mode = mode\n",
    "#         self.transition = {0: 0, 2: 0, 4: 7, 7: 7, 1: 1, 9: 1, 3: 5, 5: 3, 6: 6,\n",
    "#                            8: 8}  # class transition for asymmetric noise for cifar10\n",
    "#         # generate asymmetric noise for cifar100\n",
    "#         self.transition_cifar100 = {}\n",
    "#         nb_superclasses = 20\n",
    "#         nb_subclasses = 5\n",
    "#         base = [1, 2, 3, 4, 0]\n",
    "#         for i in range(nb_superclasses * nb_subclasses):\n",
    "#             self.transition_cifar100[i] = int(base[i % 5] + 5 * int(i / 5))\n",
    "#\n",
    "#         if self.mode == 'test':\n",
    "#             if dataset == 'cifar10':\n",
    "#                 test_dic = unpickle('%s/test_batch' % root_dir)\n",
    "#                 self.test_data = test_dic['data']\n",
    "#                 self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
    "#                 self.test_data = self.test_data.transpose((0, 2, 3, 1))\n",
    "#                 self.test_label = test_dic['labels']\n",
    "#             elif dataset == 'cifar100':\n",
    "#                 test_dic = unpickle('%s/test' % root_dir)\n",
    "#                 self.test_data = test_dic['data']\n",
    "#                 self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
    "#                 self.test_data = self.test_data.transpose((0, 2, 3, 1))\n",
    "#                 self.test_label = test_dic['fine_labels']\n",
    "#         else:\n",
    "#             train_data = []\n",
    "#             train_label = []\n",
    "#             if dataset == 'cifar10':\n",
    "#                 for n in range(1, 6):\n",
    "#                     dpath = '%s/data_batch_%d' % (root_dir, n)\n",
    "#                     data_dic = unpickle(dpath)\n",
    "#                     train_data.append(data_dic['data'])\n",
    "#                     train_label = train_label + data_dic['labels']\n",
    "#                 train_data = np.concatenate(train_data)\n",
    "#             elif dataset == 'cifar100':\n",
    "#                 train_dic = unpickle('%s/train' % root_dir)\n",
    "#                 train_data = train_dic['data']\n",
    "#                 train_label = train_dic['fine_labels']\n",
    "#                 # print(train_label)\n",
    "#                 # print(len(train_label))\n",
    "#             train_data = train_data.reshape((50000, 3, 32, 32))\n",
    "#             train_data = train_data.transpose((0, 2, 3, 1))\n",
    "#\n",
    "#             noise_label = json.load(open(noise_file, \"r\"))\n",
    "#\n",
    "#             if self.mode == 'train':\n",
    "#                 self.train_data = train_data\n",
    "#                 self.noise_label = noise_label\n",
    "#                 self.clean_label = train_label\n",
    "#\n",
    "#     def __getitem__(self, index):\n",
    "#         if self.mode == 'train':\n",
    "#             img, target = self.train_data[index], self.noise_label[index]\n",
    "#             img = Image.fromarray(img)\n",
    "#             img = self.transform(img)\n",
    "#             return img, target, index\n",
    "#         elif self.mode == 'test':\n",
    "#             img, target = self.test_data[index], self.test_label[index]\n",
    "#             img = Image.fromarray(img)\n",
    "#             img = self.transform(img)\n",
    "#             return img, target\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         if self.mode != 'test':\n",
    "#             return len(self.train_data)\n",
    "#         else:\n",
    "#             return len(self.test_data)\n",
    "#\n",
    "#\n",
    "# class cifar_dataloader():\n",
    "#     def __init__(self, dataset, batch_size, num_workers, root_dir, noise_file=''):\n",
    "#         self.dataset = dataset\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_workers = num_workers\n",
    "#         self.root_dir = root_dir\n",
    "#         self.noise_file = noise_file\n",
    "#         if self.dataset == 'cifar10':\n",
    "#             self.transform_train = transforms.Compose([\n",
    "#                 transforms.RandomCrop(32, padding=4),\n",
    "#                 transforms.RandomHorizontalFlip(),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "#             ])\n",
    "#             self.transform_test = transforms.Compose([\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "#             ])\n",
    "#         elif self.dataset == 'cifar100':\n",
    "#             self.transform_train = transforms.Compose([\n",
    "#                 transforms.RandomCrop(32, padding=4),\n",
    "#                 transforms.RandomHorizontalFlip(),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276)),\n",
    "#             ])\n",
    "#             self.transform_test = transforms.Compose([\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276)),\n",
    "#             ])\n",
    "#\n",
    "#     def run(self, mode):\n",
    "#         if mode == 'train':\n",
    "#             train_dataset = cifarDataset(dataset=self.dataset,\n",
    "#                                          root_dir=self.root_dir, transform=self.transform_train, mode=\"train\",\n",
    "#                                          noise_file=self.noise_file)\n",
    "#             trainloader = DataLoader(\n",
    "#                 dataset=train_dataset,\n",
    "#                 batch_size=self.batch_size,\n",
    "#                 shuffle=True,\n",
    "#                 num_workers=self.num_workers)\n",
    "#             return trainloader, np.asarray(train_dataset.noise_label), np.asarray(train_dataset.clean_label)\n",
    "#\n",
    "#         elif mode == 'test':\n",
    "#             test_dataset = cifarDataset(dataset=self.dataset,\n",
    "#                                         root_dir=self.root_dir, transform=self.transform_test, mode='test')\n",
    "#             test_loader = DataLoader(\n",
    "#                 dataset=test_dataset,\n",
    "#                 batch_size=self.batch_size,\n",
    "#                 shuffle=False,\n",
    "#                 num_workers=self.num_workers)\n",
    "#             return test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "# test the custom loaders for CIFAR\n",
    "dataset = 'cifar10'  # either cifar10 or cifar100\n",
    "data_path = 'rawdata_CIFAR10'  # path to the data file (don't forget to download the feature data and also put the noisy label file under this folder)\n",
    "\n",
    "num_iter_per_epoch = 100\n",
    "num_print_freq = 100\n",
    "num_epoch = 200\n",
    "num_batch_size = 128\n",
    "num_gradual = 10\n",
    "num_exponent = 1\n",
    "num_forget_rate = 0.1\n",
    "num_noise_rate = 0.2\n",
    "num_workers = 4\n",
    "num_classes = 10\n",
    "num_learning_rate = 0.001\n",
    "num_epoch_decay_start = 80\n",
    "num_input_channel = 3\n",
    "\n",
    "num_mixup_alpha = 0.1\n",
    "\n",
    "# Adjust learning rate and betas for Adam Optimizer\n",
    "mom1 = 0.9\n",
    "mom2 = 0.1\n",
    "alpha_plan = [num_learning_rate] * num_epoch\n",
    "beta1_plan = [mom1] * num_epoch\n",
    "for i in range(num_epoch_decay_start, num_epoch):\n",
    "    alpha_plan[i] = float(num_epoch - i) / (num_epoch - num_epoch_decay_start) * num_learning_rate\n",
    "    beta1_plan[i] = mom2\n",
    "\n",
    "rate_schedule = np.ones(num_epoch) * num_forget_rate\n",
    "rate_schedule[:num_gradual] = np.linspace(0, num_forget_rate ** num_exponent, num_gradual)\n",
    "\n",
    "json_noise_file_names = {\n",
    "    1: 'cifar10_noisy_labels_task1.json',\n",
    "    2: 'cifar10_noisy_labels_task2.json',\n",
    "    3: 'cifar10_noisy_labels_task3.json'\n",
    "}\n",
    "noise_file_name = json_noise_file_names[1]\n",
    "\n",
    "loader = CifarDataloader(dataset, batch_size=128,\n",
    "                          num_workers=10,\n",
    "                          root_dir=data_path,\n",
    "                          noise_file='%s/%s' % (data_path, noise_file_name))\n",
    "\n",
    "all_trainloader, noisy_labels, clean_labels = loader.run('train')\n",
    "noise_or_not = np.transpose(noisy_labels) == np.transpose(clean_labels)\n",
    "test_loader = loader.run('test')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_full_name = f'comixing_{dataset}_{noise_file_name}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr']=alpha_plan[epoch]\n",
    "        param_group['betas']=(beta1_plan[epoch], 0.999) # Only change beta1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# TODO: Complete migrating the Co-Teaching model\n",
    "def accuracy(logit, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    output = F.softmax(logit, dim=1)\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def train(loader_train, epoch, model1, optimizer1, model2, optimizer2, criterion):\n",
    "    print(\"Training... %s\" % model_full_name)\n",
    "    pure_ratio_list = []\n",
    "    pure_ratio_1_list = []\n",
    "    pure_ratio_2_list = []\n",
    "    count_total_train1 = 0\n",
    "    count_total_correct1 = 0\n",
    "    count_total_train2 = 0\n",
    "    count_total_correct2 = 0\n",
    "    num_correct_1 = 0\n",
    "    num_correct_2 = 0\n",
    "    num_total = 0\n",
    "\n",
    "    for i, (images, labels, indexes) in enumerate(loader_train):\n",
    "        ind = indexes.cpu().numpy().transpose()\n",
    "        if i > num_iter_per_epoch:\n",
    "            break\n",
    "\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        num_total = labels.size(0)\n",
    "\n",
    "        # TODO: Implement Mixup for data input\n",
    "        images, label_a, label_b, lam = mixup_data(images, labels, num_mixup_alpha, True)\n",
    "\n",
    "        # Forward Backward Optimize\n",
    "        output1 = model1(images)\n",
    "        _, predicted1 = torch.max(output1.data, 1)\n",
    "        prec1, _ = accuracy(output1, labels, topk=(1, 5))\n",
    "        count_total_train1 += 1\n",
    "        count_total_correct1 += prec1\n",
    "\n",
    "        output2 = model2(images)\n",
    "        _, predicted2 = torch.max(output2.data, 1)\n",
    "        prec2, _ = accuracy(output2, labels, topk=(1, 5))\n",
    "        count_total_train2 += 1\n",
    "        count_total_correct2 += prec2\n",
    "\n",
    "        num_correct_1 += (lam * predicted1.eq(label_a.data).cpu().sum().float()\n",
    "                    + (1 - lam) * predicted1.eq(label_b.data).cpu().sum().float())\n",
    "        num_correct_2 += (lam * predicted2.eq(label_a.data).cpu().sum().float()\n",
    "                          + (1 - lam) * predicted2.eq(label_b.data).cpu().sum().float())\n",
    "        num_acc_1 = num_correct_1 / num_total\n",
    "        num_acc_2 = num_correct_2 / num_total\n",
    "\n",
    "\n",
    "        loss1, loss2, pure_ratio_1, pure_ratio_2 = loss_coteaching(criterion, output1, output2, labels, label_a, label_b, rate_schedule[epoch], ind, noise_or_not, lam)\n",
    "        pure_ratio_1_list.append(100*pure_ratio_1)\n",
    "        pure_ratio_2_list.append(100*pure_ratio_2)\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        if (i + 1) % num_print_freq == 0:\n",
    "            print(\n",
    "                'Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f, Pure Ratio1: %.4f, Pure Ratio2 %.4f'\n",
    "                % (epoch + 1, num_epoch, i + 1, len(all_trainloader) // num_batch_size, num_acc_1, num_acc_2, loss1.item(),\n",
    "                   loss2.item(), np.sum(pure_ratio_1_list) / len(pure_ratio_1_list),\n",
    "                   np.sum(pure_ratio_2_list) / len(pure_ratio_2_list)))\n",
    "\n",
    "    train_acc1 = float(count_total_correct1) / float(count_total_train1)\n",
    "    train_acc2 = float(count_total_correct2) / float(count_total_train2)\n",
    "    return train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list\n",
    "\n",
    "\n",
    "def evaluate(test_loader, model1, model2):\n",
    "    print('Evaluating %s...' % model_full_name)\n",
    "    model1 = model1.to(device)  # Change model to 'eval' mode.\n",
    "    model2 = model2.to(device)\n",
    "    correct1 = 0\n",
    "    total1 = 0\n",
    "    print(\"Start evaluating model 1\")\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = Variable(images).to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits1 = model1(images)\n",
    "        outputs1 = F.softmax(logits1, dim=1)\n",
    "        _, pred1 = torch.max(outputs1.data, 1)\n",
    "        total1 += labels.size(0)\n",
    "        correct1 += (pred1 == labels).sum()\n",
    "\n",
    "    print(\"Start evaluating model 2\")\n",
    "    model2.eval()  # Change model to 'eval' mode\n",
    "    correct2 = 0\n",
    "    total2 = 0\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = Variable(images).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits2 = model2(images)\n",
    "        outputs2 = F.softmax(logits2, dim=1)\n",
    "        _, pred2 = torch.max(outputs2.data, 1)\n",
    "        total2 += labels.size(0)\n",
    "        correct2 += (pred2 == labels).sum()\n",
    "\n",
    "    acc1 = 100 * float(correct1) / float(total1)\n",
    "    acc2 = 100 * float(correct2) / float(total2)\n",
    "    return acc1, acc2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "print('loading dataset...')\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "#                                            batch_size=num_batch_size,\n",
    "#                                            num_workers=num_workers,\n",
    "#                                            drop_last=True,\n",
    "#                                            shuffle=True)\n",
    "#\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "#                                           batch_size=num_batch_size,\n",
    "#                                           num_workers=num_workers,\n",
    "#                                           drop_last=True,\n",
    "#                                           shuffle=False)\n",
    "\n",
    "train_loader = all_trainloader\n",
    "\n",
    "# Define models\n",
    "print('building model...')\n",
    "cnn1 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "cnn1.to(device)\n",
    "# print(cnn1.parameters)\n",
    "optimizer1 = torch.optim.Adam(cnn1.parameters(), lr=num_learning_rate)\n",
    "\n",
    "cnn2 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "cnn2.to(device)\n",
    "# print(cnn2.parameters)\n",
    "optimizer2 = torch.optim.Adam(cnn2.parameters(), lr=num_learning_rate)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduce=False)\n",
    "mean_pure_ratio1=0\n",
    "mean_pure_ratio2=0\n",
    "\n",
    "# with open(txtfile, \"a\") as myfile:\n",
    "#     myfile.write('epoch: train_acc1 train_acc2 test_acc1 test_acc2 pure_ratio1 pure_ratio2\\n')\n",
    "\n",
    "epoch=0\n",
    "train_acc1=0\n",
    "train_acc2=0\n",
    "# evaluate models with random weights\n",
    "# test_acc1, test_acc2=evaluate(test_loader, cnn1, cnn2)\n",
    "# print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %% Pure Ratio1 %.4f %% Pure Ratio2 %.4f %%' % (epoch+1, num_epoch, len(test_loader.dataset), test_acc1, test_acc2, mean_pure_ratio1, mean_pure_ratio2))\n",
    "# save results\n",
    "# with open(txtfile, \"a\") as myfile:\n",
    "#     myfile.write(str(int(epoch)) + ': '  + str(train_acc1) +' '  + str(train_acc2) +' '  + str(test_acc1) + \" \" + str(test_acc2) + ' '  + str(mean_pure_ratio1) + ' '  + str(mean_pure_ratio2) + \"\\n\")\n",
    "\n",
    "\n",
    "# training\n",
    "for epoch in range(1, num_epoch):\n",
    "    # train models\n",
    "    cnn1.train()\n",
    "    adjust_learning_rate(optimizer1, epoch)\n",
    "    cnn2.train()\n",
    "    adjust_learning_rate(optimizer2, epoch)\n",
    "    train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list=train(train_loader, epoch, cnn1, optimizer1, cnn2, optimizer2, criterion)\n",
    "    # evaluate models\n",
    "    test_acc1, test_acc2=evaluate(test_loader, cnn1, cnn2)\n",
    "    # save results\n",
    "    mean_pure_ratio1 = sum(pure_ratio_1_list)/len(pure_ratio_1_list)\n",
    "    mean_pure_ratio2 = sum(pure_ratio_2_list)/len(pure_ratio_2_list)\n",
    "    print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %%, Pure Ratio 1 %.4f %%, Pure Ratio 2 %.4f %%' % (epoch+1, num_epoch, len(test_loader.dataset), test_acc1, test_acc2, mean_pure_ratio1, mean_pure_ratio2))\n",
    "    # with open(txtfile, \"a\") as myfile:\n",
    "    #     myfile.write(str(int(epoch)) + ': '  + str(train_acc1) +' '  + str(train_acc2) +' '  + str(test_acc1) + \" \" + str(test_acc2) + ' ' + str(mean_pure_ratio1) + ' ' + str(mean_pure_ratio2) + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}