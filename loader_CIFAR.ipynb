{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from model import CNN\n",
    "from loss import loss_coteaching\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mixup import mixup_data\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from loader_CIFAR import CifarDataloader, CifarDataset, unpickle\n",
    "from loader_ANIMAL10N import Animal10N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "# test the custom loaders for CIFAR\n",
    "dataset = 'animal10n'  # either cifar10 or animal10n\n",
    "data_path_animal = 'rawdata_ANIMAL10N'\n",
    "data_path = 'rawdata_CIFAR10'  # path to the data file (don't forget to download the feature data and also put the noisy label file under this folder)\n",
    "\n",
    "num_iter_per_epoch = 100\n",
    "num_print_freq = 100\n",
    "num_epoch = 200\n",
    "num_batch_size = 16\n",
    "num_gradual = 10\n",
    "num_exponent = 1\n",
    "num_forget_rate = 0.1\n",
    "num_noise_rate = 0.2\n",
    "num_workers = 8\n",
    "num_classes = 10\n",
    "num_learning_rate = 0.001\n",
    "num_epoch_decay_start = 80\n",
    "num_input_channel = 3\n",
    "\n",
    "num_mixup_alpha = 0.1\n",
    "\n",
    "# Adjust learning rate and betas for Adam Optimizer\n",
    "mom1 = 0.9\n",
    "mom2 = 0.1\n",
    "alpha_plan = [num_learning_rate] * num_epoch\n",
    "beta1_plan = [mom1] * num_epoch\n",
    "for i in range(num_epoch_decay_start, num_epoch):\n",
    "    alpha_plan[i] = float(num_epoch - i) / (num_epoch - num_epoch_decay_start) * num_learning_rate\n",
    "    beta1_plan[i] = mom2\n",
    "\n",
    "rate_schedule = np.ones(num_epoch) * num_forget_rate\n",
    "rate_schedule[:num_gradual] = np.linspace(0, num_forget_rate ** num_exponent, num_gradual)\n",
    "\n",
    "json_noise_file_names = {\n",
    "    1: 'cifar10_noisy_labels_task1.json',\n",
    "    2: 'cifar10_noisy_labels_task2.json',\n",
    "    3: 'cifar10_noisy_labels_task3.json'\n",
    "}\n",
    "noise_file_name = json_noise_file_names[1]\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    loader = CifarDataloader(dataset, batch_size=128,\n",
    "                             num_workers=10,\n",
    "                             root_dir=data_path,\n",
    "                             noise_file='%s/%s' % (data_path, noise_file_name))\n",
    "    train_loader, noisy_labels, clean_labels = loader.run('train')\n",
    "    noise_or_not = np.transpose(noisy_labels) == np.transpose(clean_labels)\n",
    "    test_loader = loader.run('test')\n",
    "else:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset_train = Animal10N(split='train', transform=transform_train)\n",
    "    dataset_test = Animal10N(split='test', transform=transform_test)\n",
    "\n",
    "    train_loader = DataLoader(dataset_train, batch_size=num_batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(dataset_test, batch_size=num_batch_size * 4, shuffle=False, num_workers=num_workers)\n",
    "    noise_or_not = None\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_full_name = f'comixing_{dataset}_{noise_file_name}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr']=alpha_plan[epoch]\n",
    "        param_group['betas']=(beta1_plan[epoch], 0.999) # Only change beta1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# TODO: Complete migrating the Co-Teaching model\n",
    "def accuracy(logit, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    output = F.softmax(logit, dim=1)\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def train(loader_train, epoch, model1, optimizer1, model2, optimizer2, criterion):\n",
    "    print(\"Training... %s\" % model_full_name)\n",
    "    pure_ratio_list = []\n",
    "    pure_ratio_1_list = []\n",
    "    pure_ratio_2_list = []\n",
    "    count_total_train1 = 0\n",
    "    count_total_correct1 = 0\n",
    "    count_total_train2 = 0\n",
    "    count_total_correct2 = 0\n",
    "    num_correct_1 = 0\n",
    "    num_correct_2 = 0\n",
    "    num_total = 0\n",
    "\n",
    "    for i, (images, labels, indexes) in enumerate(loader_train):\n",
    "        ind = indexes.cpu().numpy().transpose()\n",
    "        if i > num_iter_per_epoch:\n",
    "            break\n",
    "\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        num_total = labels.size(0)\n",
    "\n",
    "        images, label_a, label_b, lam = mixup_data(images, labels, num_mixup_alpha, True)\n",
    "\n",
    "        # Forward Backward Optimize\n",
    "        output1 = model1(images)\n",
    "        _, predicted1 = torch.max(output1.data, 1)\n",
    "        prec1, _ = accuracy(output1, labels, topk=(1, 5))\n",
    "        count_total_train1 += 1\n",
    "        count_total_correct1 += prec1\n",
    "\n",
    "        output2 = model2(images)\n",
    "        _, predicted2 = torch.max(output2.data, 1)\n",
    "        prec2, _ = accuracy(output2, labels, topk=(1, 5))\n",
    "        count_total_train2 += 1\n",
    "        count_total_correct2 += prec2\n",
    "\n",
    "        num_correct_1 += (lam * predicted1.eq(label_a.data).cpu().sum().float()\n",
    "                    + (1 - lam) * predicted1.eq(label_b.data).cpu().sum().float())\n",
    "        num_correct_2 += (lam * predicted2.eq(label_a.data).cpu().sum().float()\n",
    "                          + (1 - lam) * predicted2.eq(label_b.data).cpu().sum().float())\n",
    "        num_acc_1 = num_correct_1 / num_total\n",
    "        num_acc_2 = num_correct_2 / num_total\n",
    "\n",
    "        loss1, loss2, pure_ratio_1, pure_ratio_2 = loss_coteaching(criterion, output1, output2, labels, label_a, label_b, rate_schedule[epoch], ind, noise_or_not, lam)\n",
    "\n",
    "        if pure_ratio_1 and pure_ratio_2 is not None:\n",
    "            pure_ratio_1_list.append(100*pure_ratio_1)\n",
    "            pure_ratio_2_list.append(100*pure_ratio_2)\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        if (i + 1) % num_print_freq == 0:\n",
    "            if pure_ratio_1 and pure_ratio_2 is not None:\n",
    "                str_calc_pure_ratio = 'Pure Ratio1: %.4f, Pure Ratio2 %.4f' % (np.sum(pure_ratio_1_list) / len(pure_ratio_1_list),\n",
    "                                                                               np.sum(pure_ratio_2_list) / len(pure_ratio_2_list))\n",
    "            else:\n",
    "                str_calc_pure_ratio = 'Animal10N dataset without pure ratio'\n",
    "\n",
    "            print(\n",
    "                'Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f, %s'\n",
    "                % (epoch + 1, num_epoch, i + 1, len(train_loader) // num_batch_size, num_acc_1, num_acc_2, loss1.item(),\n",
    "                   loss2.item(), str_calc_pure_ratio))\n",
    "\n",
    "    train_acc1 = float(count_total_correct1) / float(count_total_train1)\n",
    "    train_acc2 = float(count_total_correct2) / float(count_total_train2)\n",
    "    return train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list\n",
    "\n",
    "\n",
    "def evaluate(test_loader, model1, model2):\n",
    "    print('Evaluating %s...' % model_full_name)\n",
    "    # model1 = model1.to(device)  # Change model to 'eval' mode.\n",
    "    # model2 = model2.to(device)\n",
    "    correct1 = 0\n",
    "    total1 = 0\n",
    "    print(\"Start evaluating model 1\")\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = Variable(images).to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits1 = model1(images)\n",
    "        outputs1 = F.softmax(logits1, dim=1)\n",
    "        _, pred1 = torch.max(outputs1.data, 1)\n",
    "        total1 += labels.size(0)\n",
    "        correct1 += (pred1 == labels).sum()\n",
    "\n",
    "    print(\"Start evaluating model 2\")\n",
    "    model2.eval()  # Change model to 'eval' mode\n",
    "    correct2 = 0\n",
    "    total2 = 0\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = Variable(images).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits2 = model2(images)\n",
    "        outputs2 = F.softmax(logits2, dim=1)\n",
    "        _, pred2 = torch.max(outputs2.data, 1)\n",
    "        total2 += labels.size(0)\n",
    "        correct2 += (pred2 == labels).sum()\n",
    "\n",
    "    acc1 = 100 * float(correct1) / float(total1)\n",
    "    acc2 = 100 * float(correct2) / float(total2)\n",
    "    return acc1, acc2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "print('loading dataset...')\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "#                                            batch_size=num_batch_size,\n",
    "#                                            num_workers=num_workers,\n",
    "#                                            drop_last=True,\n",
    "#                                            shuffle=True)\n",
    "#\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "#                                           batch_size=num_batch_size,\n",
    "#                                           num_workers=num_workers,\n",
    "#                                           drop_last=True,\n",
    "#                                           shuffle=False)\n",
    "\n",
    "\n",
    "# Define models\n",
    "print('building model...')\n",
    "cnn1 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "cnn1.to(device)\n",
    "# print(cnn1.parameters)\n",
    "optimizer1 = torch.optim.Adam(cnn1.parameters(), lr=num_learning_rate)\n",
    "\n",
    "cnn2 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "cnn2.to(device)\n",
    "# print(cnn2.parameters)\n",
    "optimizer2 = torch.optim.Adam(cnn2.parameters(), lr=num_learning_rate)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduce=False)\n",
    "mean_pure_ratio1=0\n",
    "mean_pure_ratio2=0\n",
    "\n",
    "# with open(txtfile, \"a\") as myfile:\n",
    "#     myfile.write('epoch: train_acc1 train_acc2 test_acc1 test_acc2 pure_ratio1 pure_ratio2\\n')\n",
    "\n",
    "epoch=0\n",
    "train_acc1=0\n",
    "train_acc2=0\n",
    "# evaluate models with random weights\n",
    "# test_acc1, test_acc2=evaluate(test_loader, cnn1, cnn2)\n",
    "# print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %% Pure Ratio1 %.4f %% Pure Ratio2 %.4f %%' % (epoch+1, num_epoch, len(test_loader.dataset), test_acc1, test_acc2, mean_pure_ratio1, mean_pure_ratio2))\n",
    "# save results\n",
    "# with open(txtfile, \"a\") as myfile:\n",
    "#     myfile.write(str(int(epoch)) + ': '  + str(train_acc1) +' '  + str(train_acc2) +' '  + str(test_acc1) + \" \" + str(test_acc2) + ' '  + str(mean_pure_ratio1) + ' '  + str(mean_pure_ratio2) + \"\\n\")\n",
    "\n",
    "\n",
    "# training\n",
    "for epoch in range(1, num_epoch):\n",
    "    # train models\n",
    "    cnn1.train()\n",
    "    adjust_learning_rate(optimizer1, epoch)\n",
    "    cnn2.train()\n",
    "    adjust_learning_rate(optimizer2, epoch)\n",
    "    train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list=train(train_loader, epoch, cnn1, optimizer1, cnn2, optimizer2, criterion)\n",
    "    # evaluate models\n",
    "    test_acc1, test_acc2=evaluate(test_loader, cnn1, cnn2)\n",
    "    # save results\n",
    "    if dataset == 'cifar10n':\n",
    "        mean_pure_ratio1 = sum(pure_ratio_1_list)/len(pure_ratio_1_list)\n",
    "        mean_pure_ratio2 = sum(pure_ratio_2_list)/len(pure_ratio_2_list)\n",
    "        str_ratio = 'Pure Ratio 1 %.4f %%, Pure Ratio 2 %.4f' % (mean_pure_ratio1, mean_pure_ratio2)\n",
    "    else:\n",
    "        str_ratio = 'animal10n without pure ratio.'\n",
    "    print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %%, %s' % (epoch+1, num_epoch, len(test_loader.dataset), test_acc1, test_acc2, str_ratio))\n",
    "    # with open(txtfile, \"a\") as myfile:\n",
    "    #     myfile.write(str(int(epoch)) + ': '  + str(train_acc1) +' '  + str(train_acc2) +' '  + str(test_acc1) + \" \" + str(test_acc2) + ' ' + str(mean_pure_ratio1) + ' ' + str(mean_pure_ratio2) + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}