{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    import _pickle as cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo, encoding='latin1')\n",
    "    return dict\n",
    "\n",
    "\n",
    "class cifar_dataset(Dataset):\n",
    "    def __init__(self, dataset, root_dir, transform, mode, noise_file=''):\n",
    "\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.transition = {0: 0, 2: 0, 4: 7, 7: 7, 1: 1, 9: 1, 3: 5, 5: 3, 6: 6,\n",
    "                           8: 8}  # class transition for asymmetric noise for cifar10\n",
    "        # generate asymmetric noise for cifar100\n",
    "        self.transition_cifar100 = {}\n",
    "        nb_superclasses = 20\n",
    "        nb_subclasses = 5\n",
    "        base = [1, 2, 3, 4, 0]\n",
    "        for i in range(nb_superclasses * nb_subclasses):\n",
    "            self.transition_cifar100[i] = int(base[i % 5] + 5 * int(i / 5))\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            if dataset == 'cifar10':\n",
    "                test_dic = unpickle('%s/test_batch' % root_dir)\n",
    "                self.test_data = test_dic['data']\n",
    "                self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
    "                self.test_data = self.test_data.transpose((0, 2, 3, 1))\n",
    "                self.test_label = test_dic['labels']\n",
    "            elif dataset == 'cifar100':\n",
    "                test_dic = unpickle('%s/test' % root_dir)\n",
    "                self.test_data = test_dic['data']\n",
    "                self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
    "                self.test_data = self.test_data.transpose((0, 2, 3, 1))\n",
    "                self.test_label = test_dic['fine_labels']\n",
    "        else:\n",
    "            train_data = []\n",
    "            train_label = []\n",
    "            if dataset == 'cifar10':\n",
    "                for n in range(1, 6):\n",
    "                    dpath = '%s/data_batch_%d' % (root_dir, n)\n",
    "                    data_dic = unpickle(dpath)\n",
    "                    train_data.append(data_dic['data'])\n",
    "                    train_label = train_label + data_dic['labels']\n",
    "                train_data = np.concatenate(train_data)\n",
    "            elif dataset == 'cifar100':\n",
    "                train_dic = unpickle('%s/train' % root_dir)\n",
    "                train_data = train_dic['data']\n",
    "                train_label = train_dic['fine_labels']\n",
    "                # print(train_label)\n",
    "                # print(len(train_label))\n",
    "            train_data = train_data.reshape((50000, 3, 32, 32))\n",
    "            train_data = train_data.transpose((0, 2, 3, 1))\n",
    "\n",
    "            noise_label = json.load(open(noise_file, \"r\"))\n",
    "\n",
    "\n",
    "            if self.mode == 'train':\n",
    "                self.train_data = train_data\n",
    "                self.noise_label = noise_label\n",
    "                self.clean_label = train_label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'train':\n",
    "            img, target = self.train_data[index], self.noise_label[index]\n",
    "            img = Image.fromarray(img)\n",
    "            img = self.transform(img)\n",
    "            return img, target, index\n",
    "        elif self.mode == 'test':\n",
    "            img, target = self.test_data[index], self.test_label[index]\n",
    "            img = Image.fromarray(img)\n",
    "            img = self.transform(img)\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode != 'test':\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)\n",
    "\n",
    "\n",
    "class cifar_dataloader():\n",
    "    def __init__(self, dataset, batch_size, num_workers, root_dir, noise_file=''):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.root_dir = root_dir\n",
    "        self.noise_file = noise_file\n",
    "        if self.dataset == 'cifar10':\n",
    "            self.transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            self.transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "        elif self.dataset == 'cifar100':\n",
    "            self.transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276)),\n",
    "            ])\n",
    "            self.transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276)),\n",
    "            ])\n",
    "\n",
    "    def run(self, mode):\n",
    "        if mode == 'train':\n",
    "            train_dataset = cifar_dataset(dataset=self.dataset,\n",
    "                                          root_dir=self.root_dir, transform=self.transform_train, mode=\"train\",\n",
    "                                          noise_file=self.noise_file)\n",
    "            trainloader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=self.num_workers)\n",
    "            return trainloader, np.asarray(train_dataset.noise_label), np.asarray(train_dataset.clean_label)\n",
    "\n",
    "        elif mode == 'test':\n",
    "            test_dataset = cifar_dataset(dataset=self.dataset,\n",
    "                                         root_dir=self.root_dir, transform=self.transform_test, mode='test')\n",
    "            test_loader = DataLoader(\n",
    "                dataset=test_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=self.num_workers)\n",
    "            return test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# test the custom loaders for CIFAR\n",
    "dataset = 'cifar10'  # either cifar10 or cifar100\n",
    "data_path = 'rawdata_CIFAR10'  # path to the data file (don't forget to download the feature data and also put the noisy label file under this folder)\n",
    "\n",
    "num_iter_per_epoch = 100\n",
    "num_print_freq = 100\n",
    "num_epoch = 200\n",
    "num_batch_size = 128\n",
    "\n",
    "json_noise_file_names = {\n",
    "    1: 'cifar10_noisy_labels_task1.json',\n",
    "    2: 'cifar10_noisy_labels_task2.json',\n",
    "    3: 'cifar10_noisy_labels_task3.json'\n",
    "}\n",
    "noise_file_name = json_noise_file_names[1]\n",
    "\n",
    "loader = cifar_dataloader(dataset, batch_size=128,\n",
    "                          num_workers=10,\n",
    "                          root_dir=data_path,\n",
    "                          noise_file='%s/%s' % (data_path, noise_file_name))\n",
    "\n",
    "all_trainloader, noisy_labels, clean_labels = loader.run('train')\n",
    "noise_or_not = np.transpose(noisy_labels) == np.transpose(clean_labels)\n",
    "test_loader = loader.run('test')\n",
    "\n",
    "model_full_name = f'comixing_{dataset}_{noise_file_name}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_coteaching(y_1, y_2, t, forget_rate, ind, noise_or_not):\n",
    "    loss_1 = F.cross_entropy(y_1, t, reduce = False)\n",
    "    ind_1_sorted = np.argsort(loss_1.data).cuda()\n",
    "    loss_1_sorted = loss_1[ind_1_sorted]\n",
    "\n",
    "    loss_2 = F.cross_entropy(y_2, t, reduce = False)\n",
    "    ind_2_sorted = np.argsort(loss_2.data).cuda()\n",
    "    loss_2_sorted = loss_2[ind_2_sorted]\n",
    "\n",
    "    remember_rate = 1 - forget_rate\n",
    "    num_remember = int(remember_rate * len(loss_1_sorted))\n",
    "\n",
    "    pure_ratio_1 = np.sum(noise_or_not[ind[ind_1_sorted[:num_remember]]])/float(num_remember)\n",
    "    pure_ratio_2 = np.sum(noise_or_not[ind[ind_2_sorted[:num_remember]]])/float(num_remember)\n",
    "\n",
    "    ind_1_update=ind_1_sorted[:num_remember]\n",
    "    ind_2_update=ind_2_sorted[:num_remember]\n",
    "    # exchange\n",
    "    loss_1_update = F.cross_entropy(y_1[ind_2_update], t[ind_2_update])\n",
    "    loss_2_update = F.cross_entropy(y_2[ind_1_update], t[ind_1_update])\n",
    "\n",
    "    return torch.sum(loss_1_update)/num_remember, torch.sum(loss_2_update)/num_remember, pure_ratio_1, pure_ratio_2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Complete migrating the Co-Teaching model\n",
    "def accuracy(logit, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    output = F.softmax(logit, dim=1)\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def train(loader_train, epoch, model1, optimizer1, model2, optimizer2,\n",
    "          num_iterations,\n",
    "          ):\n",
    "    print(\"Training... %s\" % model_full_name)\n",
    "    pure_ratio_list=[]\n",
    "    pure_ratio_1_list=[]\n",
    "    pure_ratio_2_list=[]\n",
    "    count_total_train1 = 0\n",
    "    count_total_correct1 = 0\n",
    "    count_total_train2 = 0\n",
    "    count_total_correct2 = 0\n",
    "\n",
    "    for i, (images, labels, indexes) in enumerate(loader_train):\n",
    "        ind = indexes.cpu().numpy().transpose()\n",
    "        if i > num_iter_per_epoch:\n",
    "            break\n",
    "\n",
    "        images = Variable(images).cuda()\n",
    "        labels = Variable(labels).cude()\n",
    "\n",
    "        # Forward Backward Optimize\n",
    "        output1 = model1(images)\n",
    "        prec1, _ = accuracy(output1, labels, topk=(1, 5))\n",
    "        count_total_train1 += 1\n",
    "        count_total_correct1 += prec1\n",
    "\n",
    "        output2 = model2(images)\n",
    "        prec2, _ = accuracy(output2, labels, topk=(1, 5))\n",
    "        count_total_train2 += 1\n",
    "        count_total_correct2 += prec2\n",
    "\n",
    "        loss1, loss2 = loss_coteaching(output1, output2, labels, rate_schedule[epoch], ind,\n",
    "                                       noise_or_not)\n",
    "        # pure_ratio_1_list.append(100*pure_ratio_1)\n",
    "        # pure_ratio_2_list.append(100*pure_ratio_2)\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        if (i+1) % num_print_freq == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f, Pure Ratio1: %.4f, Pure Ratio2 %.4f'\n",
    "                   %(epoch+1, num_epoch, i+1, len(train_dataset)//num_batch_size, prec1, prec2, loss1.data[0], loss2.data[0], np.sum(pure_ratio_1_list)/len(pure_ratio_1_list), np.sum(pure_ratio_2_list)/len(pure_ratio_2_list)))\n",
    "\n",
    "    train_acc1=float(count_total_correct1)/float(count_total_train1)\n",
    "    train_acc2=float(count_total_correct2)/float(count_total_train2)\n",
    "    return train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}