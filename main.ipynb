{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from model import CNN, ReshapedPreTrainedModel\n",
    "from loss import loss_coteaching, loss_coteaching_update\n",
    "from optuna.trial import TrialState\n",
    "import optuna\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mixup import mixup_data\n",
    "from model import dict_models, load_pretrained_model_by_name\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from loader_CIFAR import CifarDataloader, CifarDataset, unpickle\n",
    "from loader_ANIMAL10N import Animal10N\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "\n",
    "# torch.autograd.set_detect_anomaly(True)  this is for debugging\n",
    "\n",
    "name_dataset = 'cifar10'  # either cifar10 or animal10n\n",
    "id_task = 3  # Task 1 to 3, noise file created by TA\n",
    "\n",
    "# path to the data file (don't forget to download the feature data and also put the noisy label file under this folder)\n",
    "if name_dataset == 'animal10n':\n",
    "    data_path = r'rawdata_ANIMAL10N'\n",
    "else:\n",
    "    data_path = r\"C://Users//Wenda//Documents//Pytorch//rawdata_CIFAR10\"\n",
    "\n",
    "num_iter_per_epoch = 100\n",
    "num_print_freq = 100\n",
    "num_epoch = 200\n",
    "\n",
    "if name_dataset == 'animal10n':\n",
    "    num_batch_size = 32  # Have to use a smaller batch for animal10n due to the GPU memory limit (GTX1080 8G)\n",
    "else:\n",
    "    num_batch_size = 128\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    num_batch_size = num_batch_size * torch.cuda.device_count()\n",
    "\n",
    "\n",
    "def tryUsingDataParallel(model):\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using parallel with {torch.cuda.device_count()} GPUs!\")\n",
    "        return torch.nn.DataParallel(model)\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "\n",
    "num_gradual = 10\n",
    "num_exponent = 1\n",
    "num_forget_rate = 0.1\n",
    "num_noise_rate = 0.2\n",
    "num_workers = 6\n",
    "num_classes = 10\n",
    "num_learning_rate = 0.001\n",
    "num_input_channel = 3\n",
    "\n",
    "# Adjust learning rate and betas for Adam Optimizer\n",
    "num_mixup_alpha = 0.1\n",
    "num_epoch_decay_start = 80\n",
    "mom1 = 0.9\n",
    "mom2 = 0.1\n",
    "alpha_plan = [num_learning_rate] * num_epoch\n",
    "beta1_plan = [mom1] * num_epoch\n",
    "for i in range(num_epoch_decay_start, num_epoch):\n",
    "    alpha_plan[i] = float(num_epoch - i) / (num_epoch - num_epoch_decay_start) * num_learning_rate\n",
    "    beta1_plan[i] = mom2\n",
    "\n",
    "rate_schedule = np.ones(num_epoch) * num_forget_rate\n",
    "rate_schedule[:num_gradual] = np.linspace(0, num_forget_rate ** num_exponent, num_gradual)\n",
    "\n",
    "json_noise_file_names = {\n",
    "    1: 'cifar10_noisy_labels_task1.json',\n",
    "    2: 'cifar10_noisy_labels_task2.json',\n",
    "    3: 'cifar10_noisy_labels_task3.json'\n",
    "}\n",
    "noise_file_name = json_noise_file_names[id_task]  # Change The number to switch tasks\n",
    "\n",
    "if name_dataset == 'cifar10':\n",
    "    loader = CifarDataloader(name_dataset, batch_size=128,\n",
    "                             num_workers=10,\n",
    "                             root_dir=data_path,\n",
    "                             noise_file='%s/%s' % (data_path, noise_file_name))\n",
    "    train_loader, noisy_labels, clean_labels = loader.run('train')\n",
    "    noise_or_not = np.transpose(noisy_labels) == np.transpose(clean_labels)\n",
    "    test_loader = loader.run('test')\n",
    "else:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset_train = Animal10N(split='train', data_path=data_path, transform=transform_train)\n",
    "    dataset_test = Animal10N(split='test', data_path=data_path, transform=transform_test)\n",
    "\n",
    "    train_loader = DataLoader(dataset_train, batch_size=num_batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(dataset_test, batch_size=num_batch_size * 2, shuffle=False, num_workers=num_workers)\n",
    "    noise_or_not = None\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if name_dataset == 'animal10n':\n",
    "    model_full_name = f'{name_dataset}'\n",
    "else:\n",
    "    model_full_name = f'{name_dataset}_{noise_file_name}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "#\n",
    "# def imshow(axis, inp):\n",
    "#     \"\"\"Denormalize and show\"\"\"\n",
    "#     inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     mean = np.array([0.485, 0.456, 0.406])\n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     inp = std * inp + mean\n",
    "#     axis.imshow(inp)\n",
    "# img, label, idx = next(iter(train_loader))\n",
    "# print(img.size(), label.size())\n",
    "# fig = plt.figure(1, figsize=(16, 4))\n",
    "# grid = ImageGrid(fig, 111, nrows_ncols=(1, 4), axes_pad=0.05)\n",
    "# for i in range(img.size()[0]):\n",
    "#     ax = grid[i]\n",
    "#     imshow(ax, img[i])\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = alpha_plan[epoch]\n",
    "        param_group['betas'] = (beta1_plan[epoch], 0.999)  # Only change beta1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# TODO: Complete migrating the Co-Teaching model\n",
    "def accuracy(logit, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    output = F.softmax(logit, dim=1)\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def train(loader_train, epoch, model1, optimizer1, model2, optimizer2, criterion, _rate_schedule, _mixup_alpha,\n",
    "          _iter_per_epoch):\n",
    "    print(\"Training... %s\" % model_full_name)\n",
    "    pure_ratio_list = []\n",
    "    _pure_ratio_1_list = []\n",
    "    _pure_ratio_2_list = []\n",
    "    count_total_train1 = 0\n",
    "    count_total_correct1 = 0\n",
    "    count_total_train2 = 0\n",
    "    count_total_correct2 = 0\n",
    "    num_correct_1 = 0\n",
    "    num_correct_2 = 0\n",
    "    num_total = 0\n",
    "    model1.to(device)\n",
    "    model2.to(device)\n",
    "\n",
    "    for i, (images, labels, indexes) in enumerate(loader_train):\n",
    "        # ind = indexes.cpu().numpy().transpose()\n",
    "        ind = indexes.numpy().transpose()\n",
    "        if i > _iter_per_epoch > 0:\n",
    "            break\n",
    "\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        num_total = labels.size(0)\n",
    "\n",
    "        images, label_a, label_b, lam = mixup_data(images, labels, device, _mixup_alpha)\n",
    "\n",
    "        # Forward Backward Optimize\n",
    "        output1 = model1(images)\n",
    "        _, predicted1 = torch.max(output1.data, 1)\n",
    "        prec1, _ = accuracy(output1, labels, topk=(1, 5))\n",
    "        count_total_train1 += 1\n",
    "        count_total_correct1 += prec1\n",
    "\n",
    "        output2 = model2(images)\n",
    "        _, predicted2 = torch.max(output2.data, 1)\n",
    "        prec2, _ = accuracy(output2, labels, topk=(1, 5))\n",
    "        count_total_train2 += 1\n",
    "        count_total_correct2 += prec2\n",
    "\n",
    "        num_correct_1 += (lam * predicted1.eq(label_a.data).cpu().sum().float()\n",
    "                          + (1 - lam) * predicted1.eq(label_b.data).cpu().sum().float())\n",
    "        num_correct_2 += (lam * predicted2.eq(label_a.data).cpu().sum().float()\n",
    "                          + (1 - lam) * predicted2.eq(label_b.data).cpu().sum().float())\n",
    "        # num_correct_1 += (lam * predicted1.eq(label_a.data).cpu().sum().float()\n",
    "        #                   + (1 - lam) * predicted1.eq(label_b.data).cpu().sum().float())\n",
    "        # num_correct_2 += (lam * predicted2.eq(label_a.data).cpu().sum().float()\n",
    "        #                   + (1 - lam) * predicted2.eq(label_b.data).cpu().sum().float())\n",
    "        num_acc_1 = num_correct_1 / num_total\n",
    "        num_acc_2 = num_correct_2 / num_total\n",
    "\n",
    "        loss1, loss2, pure_ratio_1, pure_ratio_2 = loss_coteaching(criterion, output1, output2, labels, label_a,\n",
    "                                                                   label_b, _rate_schedule[epoch], ind, noise_or_not,\n",
    "                                                                   lam)\n",
    "\n",
    "        if pure_ratio_1 and pure_ratio_2 is not None:\n",
    "            _pure_ratio_1_list.append(100 * pure_ratio_1)\n",
    "            _pure_ratio_2_list.append(100 * pure_ratio_2)\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        if (i + 1) % num_print_freq == 0:\n",
    "            if pure_ratio_1 and pure_ratio_2 is not None:\n",
    "                str_calc_pure_ratio = 'Pure Ratio1: %.4f, Pure Ratio2 %.4f' % (\n",
    "                    np.sum(_pure_ratio_1_list) / len(_pure_ratio_1_list),\n",
    "                    np.sum(_pure_ratio_2_list) / len(_pure_ratio_2_list))\n",
    "            else:\n",
    "                str_calc_pure_ratio = 'Animal10N dataset without pure ratio'\n",
    "\n",
    "            print(\n",
    "                'Train Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f, %s'\n",
    "                % (epoch + 1, num_epoch, i + 1, len(train_loader) // num_batch_size, num_acc_1, num_acc_2, loss1.item(),\n",
    "                   loss2.item(), str_calc_pure_ratio))\n",
    "\n",
    "    train_acc1 = float(count_total_correct1) / float(count_total_train1)\n",
    "    train_acc2 = float(count_total_correct2) / float(count_total_train2)\n",
    "    return train_acc1, train_acc2, _pure_ratio_1_list, _pure_ratio_2_list\n",
    "\n",
    "\n",
    "def train_improve(loader_train, epoch, model1, optimizer1, model2, optimizer2, criterion, _rate_schedule, _mixup_alpha,\n",
    "          _iter_per_epoch):\n",
    "    print(\"Training... %s\" % model_full_name)\n",
    "    pure_ratio_list = []\n",
    "    _pure_ratio_1_list = []\n",
    "    _pure_ratio_2_list = []\n",
    "    count_total_train1 = 0\n",
    "    count_total_correct1 = 0\n",
    "    count_total_train2 = 0\n",
    "    count_total_correct2 = 0\n",
    "    num_correct_1 = 0\n",
    "    num_correct_2 = 0\n",
    "    num_acc_1 = 0\n",
    "    num_acc_2 = 0\n",
    "    num_total = 0\n",
    "    num_total_1 = 0\n",
    "    num_total_2 = 0\n",
    "    model1.to(device)\n",
    "    model2.to(device)\n",
    "\n",
    "    for i, (images, labels, indexes) in enumerate(loader_train):\n",
    "        # ind = indexes.cpu().numpy().transpose()\n",
    "        ind = indexes.numpy().transpose()\n",
    "        if i > _iter_per_epoch > 0:\n",
    "            break\n",
    "\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        # num_total = labels.size(0)\n",
    "\n",
    "        # Forward Backward Optimize\n",
    "        output1 = model1(images)\n",
    "        output2 = model2(images)\n",
    "\n",
    "        loss1, loss2, pure_ratio_1, pure_ratio_2,\\\n",
    "        lam_2, lam_1, \\\n",
    "        images_1, images_2,\\\n",
    "        label_1_a, label_1_b,\\\n",
    "        label_2_a, label_2_b,\\\n",
    "            output1_mix, output2_mix = loss_coteaching_update(criterion,\n",
    "                                                      model1, model2,\n",
    "                                                      output1, output2,\n",
    "                                                      images,\n",
    "                                                      labels,\n",
    "                                                      _rate_schedule[epoch],\n",
    "                                                      ind,\n",
    "                                                      noise_or_not,\n",
    "                                                      _mixup_alpha, device)\n",
    "\n",
    "        num_total_1 += label_2_a.size(0)\n",
    "        num_total_2 += label_1_a.size(0)\n",
    "\n",
    "        # Forward Backward Optimize\n",
    "        _, predicted1 = torch.max(output1_mix.data, 1)\n",
    "        _, predicted2 = torch.max(output2_mix.data, 1)\n",
    "\n",
    "        num_correct_1 += (lam_2 * predicted1.eq(label_2_a.data).cpu().sum().float()\n",
    "                          + (1 - lam_2) * predicted1.eq(label_2_b.data).cpu().sum().float())\n",
    "        num_correct_2 += (lam_1 * predicted2.eq(label_1_a.data).cpu().sum().float()\n",
    "                          + (1 - lam_1) * predicted2.eq(label_1_b.data).cpu().sum().float())\n",
    "        num_acc_1 = num_correct_1 / num_total_1\n",
    "        num_acc_2 = num_correct_2 / num_total_2\n",
    "\n",
    "        if pure_ratio_1 and pure_ratio_2 is not None:\n",
    "            _pure_ratio_1_list.append(100 * pure_ratio_1)\n",
    "            _pure_ratio_2_list.append(100 * pure_ratio_2)\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        if (i + 1) % num_print_freq == 0:\n",
    "            if pure_ratio_1 and pure_ratio_2 is not None:\n",
    "                str_calc_pure_ratio = 'Pure Ratio1: %.4f, Pure Ratio2 %.4f' % (\n",
    "                    np.sum(_pure_ratio_1_list) / len(_pure_ratio_1_list),\n",
    "                    np.sum(_pure_ratio_2_list) / len(_pure_ratio_2_list))\n",
    "            else:\n",
    "                str_calc_pure_ratio = 'Animal10N dataset without pure ratio'\n",
    "\n",
    "            print(\n",
    "                'Train Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F (%d/%d), Training Accuracy2: %.4f (%d/%d), Loss1: %.4f, Loss2: %.4f, %s'\n",
    "                % (epoch + 1, num_epoch, i + 1, len(train_loader) // num_batch_size, num_acc_1, num_correct_1, num_total_1, num_acc_2, num_correct_2, num_total_2, loss1.item(),\n",
    "                   loss2.item(), str_calc_pure_ratio))\n",
    "\n",
    "    return num_acc_1, num_acc_2, _pure_ratio_1_list, _pure_ratio_2_list\n",
    "\n",
    "\n",
    "def evaluate(_test_loader, model1, model2, particial_test_rate=0):\n",
    "    print('Evaluating %s...' % model_full_name)\n",
    "    # model1 = model1.to(device)\n",
    "    # model2 = model2.to(device)\n",
    "    model1.eval()  # Change model to 'eval' mode.\n",
    "    correct1 = 0\n",
    "    total1 = 0\n",
    "    print(\"Start evaluating model 1\")\n",
    "    for images, labels in _test_loader:\n",
    "        images = Variable(images).to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits1 = model1(images)\n",
    "        outputs1 = F.softmax(logits1, dim=1)\n",
    "        _, pred1 = torch.max(outputs1.data, 1)\n",
    "        total1 += labels.size(0)\n",
    "        correct1 += (pred1 == labels).sum()\n",
    "\n",
    "    print(\"Start evaluating model 2\")\n",
    "    model2.eval()  # Change model to 'eval' mode\n",
    "    correct2 = 0\n",
    "    total2 = 0\n",
    "    for images, labels in _test_loader:\n",
    "        images = Variable(images).to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits2 = model2(images)\n",
    "        outputs2 = F.softmax(logits2, dim=1)\n",
    "        _, pred2 = torch.max(outputs2.data, 1)\n",
    "        total2 += labels.size(0)\n",
    "        correct2 += (pred2 == labels).sum()\n",
    "\n",
    "    acc1 = 100 * float(correct1) / float(total1)\n",
    "    acc2 = 100 * float(correct2) / float(total2)\n",
    "    return acc1, acc2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "\n",
    "def main_single_train():\n",
    "    # Train the model\n",
    "\n",
    "    # Data Loader (Input Pipeline)\n",
    "    print('loading dataset...')\n",
    "\n",
    "    # Define models\n",
    "    print('building model...')\n",
    "    cnn1 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "    # cnn1 = ReshapedPreTrainedModel(modelC, n_outputs=10, dropout_rate=0.25, freeze_weights=True)\n",
    "    cnn1 = tryUsingDataParallel(cnn1)\n",
    "    cnn1.to(device)\n",
    "    # print(cnn1.parameters)\n",
    "    optimizer1 = torch.optim.Adam(cnn1.parameters(), lr=num_learning_rate)\n",
    "\n",
    "    cnn2 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "    # cnn2 = ReshapedPreTrainedModel(modelC, n_outputs=10, dropout_rate=0.25, freeze_weights=True)\n",
    "    cnn2 = tryUsingDataParallel(cnn2)\n",
    "    cnn2.to(device)\n",
    "    # print(cnn2.parameters)\n",
    "    optimizer2 = torch.optim.Adam(cnn2.parameters(), lr=num_learning_rate)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduce=False)\n",
    "\n",
    "    # training\n",
    "    for epoch in range(1, num_epoch):\n",
    "        # train models\n",
    "        cnn1.train()\n",
    "        adjust_learning_rate(optimizer1, epoch)\n",
    "        cnn2.train()\n",
    "        adjust_learning_rate(optimizer2, epoch)\n",
    "        train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list = train(train_loader, epoch, cnn1, optimizer1,\n",
    "                                                                             cnn2, optimizer2, criterion, rate_schedule,\n",
    "                                                                             num_mixup_alpha, num_iter_per_epoch)\n",
    "        # evaluate models\n",
    "        test_acc1, test_acc2 = evaluate(test_loader, cnn1, cnn2)\n",
    "        # save results\n",
    "        if name_dataset == 'cifar10n':\n",
    "            mean_pure_ratio1 = sum(pure_ratio_1_list) / len(pure_ratio_1_list)\n",
    "            mean_pure_ratio2 = sum(pure_ratio_2_list) / len(pure_ratio_2_list)\n",
    "            str_ratio = 'Pure Ratio 1 %.4f %%, Pure Ratio 2 %.4f' % (mean_pure_ratio1, mean_pure_ratio2)\n",
    "        else:\n",
    "            str_ratio = 'animal10n without pure ratio.'\n",
    "        print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %%, %s' % (\n",
    "            epoch + 1, num_epoch, len(test_loader.dataset), test_acc1, test_acc2, str_ratio))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def train_trial(_model_select,\n",
    "                _optimizer_select,\n",
    "                _learning_rate_scheduler_select,\n",
    "                trial,\n",
    "                _forget_rate=0.1, _gradual=10, _num_epoch=200, _num_mixup_alpha=0.1, _iter_per_epoch=200,\n",
    "                _num_exponent=1, _learning_rate=0.02, _freq_evaluate=1, _freeze_weights=True, _train_improve=False):\n",
    "    _rate_schedule = np.ones(_num_epoch) * _forget_rate\n",
    "    _rate_schedule[:_gradual] = np.linspace(0, _forget_rate ** _num_exponent, _gradual)\n",
    "\n",
    "    print('Start trail...')\n",
    "    # Define models\n",
    "    print('building model...')\n",
    "    if _model_select != \"CNN\":\n",
    "        print(f'Loading pre-trained model: {_model_select}')\n",
    "        _model1 = load_pretrained_model_by_name(_model_select, is_pretrained=True)\n",
    "        _model2 = load_pretrained_model_by_name(_model_select, is_pretrained=True)\n",
    "\n",
    "        if _freeze_weights:\n",
    "            for param in _model1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in _model2.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_input = _model1.classifier.in_features\n",
    "        _model1.classifier = torch.nn.Linear(num_input, num_classes, bias=True).to(device)\n",
    "        num_input = _model2.classifier.in_features\n",
    "        _model2.classifier = torch.nn.Linear(num_input, num_classes, bias=True).to(device)\n",
    "        # _model1 = modify_pretrained_outputs(_model1.to(device), num_output=num_classes, freeze_parameters=_freeze_weights)\n",
    "        # _model1 = modify_pretrained_outputs(_model2.to(device), num_output=num_classes, freeze_parameters=_freeze_weights)\n",
    "    else:\n",
    "        _model1 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "        _model2 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "\n",
    "    _model1 = tryUsingDataParallel(_model1)\n",
    "    _model2 = tryUsingDataParallel(_model2)\n",
    "    _model1.to(device)\n",
    "    _model2.to(device)\n",
    "\n",
    "    if _optimizer_select == \"Adam\":\n",
    "        _optm1 = torch.optim.Adam(_model1.parameters(), lr=_learning_rate)\n",
    "        _optm2 = torch.optim.Adam(_model2.parameters(), lr=_learning_rate)\n",
    "    else:\n",
    "        _optm1 = torch.optim.SGD(_model1.parameters(), lr=_learning_rate)\n",
    "        _optm2 = torch.optim.SGD(_model2.parameters(), lr=_learning_rate)\n",
    "\n",
    "    if _learning_rate_scheduler_select == \"StepLR\":\n",
    "        scheduler1 = lr_scheduler.StepLR(_optm1, step_size=30, gamma=0.5)\n",
    "        scheduler2 = lr_scheduler.StepLR(_optm2, step_size=30, gamma=0.5)\n",
    "\n",
    "    _criterion_select = torch.nn.CrossEntropyLoss(reduce=False)\n",
    "    _mean_pure_ratio1 = 0\n",
    "    _mean_pure_ratio2 = 0\n",
    "    _epoch = 0\n",
    "    _train_acc1 = 0\n",
    "    _train_acc2 = 0\n",
    "    _test_acc1, _test_acc2 = 0, 0\n",
    "\n",
    "    print(\n",
    "        f\"Current model: {_model_select}, current optimizer: {_optimizer_select}, criterion: {_criterion_select}, lr schedule: {_learning_rate_scheduler_select}, forget rate: {_forget_rate}, gradual rate: {_gradual}, mixup alpha: {_num_mixup_alpha}\")\n",
    "\n",
    "    for _epoch in range(_num_epoch):\n",
    "        # train models\n",
    "        _model1.train()\n",
    "        _model2.train()\n",
    "\n",
    "        if _train_improve:\n",
    "            _train_acc1, _train_acc2, _pure_ratio_1_list, _pure_ratio_2_list = train_improve(train_loader, _epoch, _model1, _optm1,\n",
    "                                                                                     _model2, _optm2, _criterion_select,\n",
    "                                                                                     _rate_schedule, _num_mixup_alpha,\n",
    "                                                                                     _iter_per_epoch)\n",
    "        else:\n",
    "            _train_acc1, _train_acc2, _pure_ratio_1_list, _pure_ratio_2_list = train(train_loader, _epoch, _model1, _optm1,\n",
    "                                                                                     _model2, _optm2, _criterion_select,\n",
    "                                                                                     _rate_schedule, _num_mixup_alpha,\n",
    "                                                                                     _iter_per_epoch)\n",
    "\n",
    "        if _learning_rate_scheduler_select == \"StepLR\":\n",
    "            scheduler1.step(_epoch)\n",
    "            scheduler2.step(_epoch)\n",
    "        else:\n",
    "            adjust_learning_rate(_optm1, _epoch)\n",
    "            adjust_learning_rate(_optm2, _epoch)\n",
    "\n",
    "        if (_epoch + 1) % _freq_evaluate == 0:\n",
    "            # evaluate models\n",
    "            _test_acc1, _test_acc2 = evaluate(test_loader, _model1, _model2)\n",
    "            # save results\n",
    "            if name_dataset == 'cifar10':\n",
    "                _mean_pure_ratio1 = sum(_pure_ratio_1_list) / len(_pure_ratio_1_list)\n",
    "                _mean_pure_ratio2 = sum(_pure_ratio_2_list) / len(_pure_ratio_2_list)\n",
    "                _str_ratio = 'Pure Ratio 1 %.4f %%, Pure Ratio 2 %.4f' % (_mean_pure_ratio1, _mean_pure_ratio2)\n",
    "            else:\n",
    "                _str_ratio = 'animal10n without pure ratio.'\n",
    "            print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %%, %s' % (\n",
    "                _epoch + 1, num_epoch, len(test_loader.dataset), _test_acc1, _test_acc2, _str_ratio))\n",
    "\n",
    "            _temp_acc = max(_test_acc1, _test_acc2)  # current best testing accuracy\n",
    "            if trial is not None:\n",
    "                trial.report(_temp_acc, _epoch)\n",
    "                # TODO: Renable this when checking default performance is done\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "                else:\n",
    "                    print(\"Trial not pruned yet\")\n",
    "            else:\n",
    "                print(\"Trial input is none, assume the study is a single test run\")\n",
    "\n",
    "    return max(_test_acc1, _test_acc2)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    #  _criterion_select = trial.suggest_categorical(\n",
    "    #      \"Criterion\",\n",
    "    #      [\"CEL\"])\n",
    "    _forget_rate_selection = trial.suggest_categorical(\n",
    "        \"Forget Rate\",\n",
    "        [0.05, 0.1, 0.2, 0.5])\n",
    "    _gradual = trial.suggest_categorical(\n",
    "        \"Gradual\",\n",
    "        [10, 50])\n",
    "    _mixup_alpha = trial.suggest_categorical(\n",
    "        \"Mixup Alpha\",\n",
    "        [0, 0.05, 0.1, 0.5])\n",
    "    _optm_select = trial.suggest_categorical(\n",
    "        \"Optimizer\",\n",
    "        [\"SGD\", \"Adam\"])\n",
    "    _lr_scheduler_select = trial.suggest_categorical(\n",
    "        \"Scheduler\",\n",
    "        [\"StepLR\", \"None\"]\n",
    "    )\n",
    "    _lr_select = trial.suggest_categorical(\n",
    "        \"Learning Rate\",\n",
    "        [0.08, 0.04, 0.02, 0.008, 0.004]\n",
    "    )\n",
    "    _model_select = trial.suggest_categorical(  # TODO: Search models for training\n",
    "        \"Model\",\n",
    "        [\"CNN\",\n",
    "         \"Densenet121\",\n",
    "         \"EfficientNet_v2_rw_t\", \"EfficientNet_v2_s\", \"EfficientNet_v2_m\", \"Mobilenet_v2_035\", \"Mobilenet_v2_100\"]\n",
    "    )\n",
    "\n",
    "    _accuracy = train_trial(_model_select,\n",
    "                            _optm_select,\n",
    "                            _lr_scheduler_select,\n",
    "                            trial=trial,\n",
    "                            _forget_rate=_forget_rate_selection, _gradual=_gradual, _num_epoch=200,\n",
    "                            _num_mixup_alpha=_mixup_alpha, _iter_per_epoch=200, _num_exponent=1, _learning_rate=_lr_select,\n",
    "                            _freq_evaluate=5, _freeze_weights=False, _train_improve=True)\n",
    "    return _accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run the study here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "study_trial = True\n",
    "\n",
    "if name_dataset == \"cifar10\":\n",
    "    study_name = f\"{name_dataset}_Task{id_task}_2022_04_17_alg_improve\"  # update the date manually here\n",
    "else:  # When the dataset is animal10n\n",
    "    study_name = f\"{name_dataset}_unfreeze_weight_nomixup\"\n",
    "\n",
    "# search_space = {\"Forget Rate\":[0.05, 0.1, 0.2, 0.5],\n",
    "#                 \"Gradual\":[10, 50],\n",
    "#                 \"Mixup Alpha\":[0, 0.05, 0.1, 0.5],\n",
    "#                 \"Optimizer\":[\"SGD\", \"Adam\"],\n",
    "#                 \"Scheduler\":[\"StepLR\", \"None\"],\n",
    "#                 \"Learning Rate\":[0.08, 0.04, 0.02, 0.008, 0.004],\n",
    "#                 \"Model\":[\"CNN\",\"Densenet121\",\"EfficientNet_v2_rw_t\"]\n",
    "#                 }\n",
    "# search_space = {\"Forget Rate\":[0.05, 0.1, 0.2, 0.5],\n",
    "#                 \"Gradual\":[10, 50],\n",
    "#                 \"Mixup Alpha\":[0, 0.05, 0.1, 0.5],\n",
    "#                 \"Optimizer\":[\"SGD\"],\n",
    "#                 \"Scheduler\":[\"StepLR\"],\n",
    "#                 \"Learning Rate\":[0.08, 0.04, 0.02, 0.008, 0.004],\n",
    "#                 \"Model\":[\"CNN\",\"Densenet121\",\"EfficientNet_v2_rw_t\"]\n",
    "#                 }\n",
    "\n",
    "\n",
    "# if study_trial:\n",
    "#     search_space = {\"Forget Rate\":[0.05, 0.1, 0.2, 0.5],\n",
    "#                     \"Gradual\":[10, 50],\n",
    "#                     \"Mixup Alpha\":[0, 0.05, 0.1, 0.5],\n",
    "#                     \"Optimizer\":[\"SGD\"],\n",
    "#                     \"Scheduler\":[\"StepLR\"],\n",
    "#                     \"Learning Rate\":[0.08, 0.04, 0.02, 0.008, 0.004],\n",
    "#                     \"Model\":[\"EfficientNet_v2_s\", \"EfficientNet_v2_m\", \"Mobilenet_v2_035\", \"Mobilenet_v2_100\"]\n",
    "#                     }\n",
    "\n",
    "# if study_trial:\n",
    "#     search_space = {\"Forget Rate\": [0.2, 0.5],\n",
    "#                     \"Gradual\": [50],\n",
    "#                     \"Mixup Alpha\": [0],\n",
    "#                     \"Optimizer\": [\"SGD\"],\n",
    "#                     \"Scheduler\": [\"StepLR\"],\n",
    "#                     \"Learning Rate\": [0.08, 0.04],\n",
    "#                     \"Model\": [\"EfficientNet_v2_m\"]\n",
    "#                     }\n",
    "\n",
    "if study_trial:\n",
    "    search_space = {\"Forget Rate\":[0.1, 0.2, 0.5, 0.7],\n",
    "                    \"Gradual\":[50],\n",
    "                    \"Mixup Alpha\":[0.05, 0.1, 0.5],\n",
    "                    \"Optimizer\":[\"SGD\"],\n",
    "                    \"Scheduler\":[\"StepLR\"],\n",
    "                    \"Learning Rate\":[0.08, 0.04, 0.02, 0.008],\n",
    "                    \"Model\":[\"CNN\"]\n",
    "                    }\n",
    "\n",
    "    # TODO: Configure MySQL DB for remote connection\n",
    "    study = optuna.create_study(study_name=study_name,\n",
    "                                storage='mysql://root:1115@192.168.50.11:3306/co_mixing',\n",
    "                                load_if_exists=True,\n",
    "                                direction=\"maximize\",\n",
    "                                sampler=optuna.samplers.GridSampler(search_space))\n",
    "\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Facts:\")\n",
    "    print(\"  Finished Trials: \", len(study.trials))\n",
    "    print(\"  Pruned Trials: \", len(pruned_trials))\n",
    "    print(\"  Complete Trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best Trial:\")\n",
    "    trial_best = study.best_trial\n",
    "    print(\"  Value: \", trial_best.value)\n",
    "    print(\"  Parameters: \")\n",
    "    for key, value in trial_best.params.items():\n",
    "        print(\"  {}: {}\".format(key, value))\n",
    "else:\n",
    "    train_trial(\"EfficientNet_v2_rw_t\",\n",
    "                \"SGD\",\n",
    "                \"StepLR\",\n",
    "                trial=None,\n",
    "                _forget_rate=0.2, _gradual=50, _num_epoch=200, _num_mixup_alpha=0.1, _iter_per_epoch=200,\n",
    "                _num_exponent=1, _learning_rate=0.04, _freq_evaluate=5, _freeze_weights=False, _train_improve=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# modelA = torch.hub.load('rwightman/pytorch-image-models', 'tf_efficientnetv2_s_in21ft1k')\n",
    "# modelB = torch.hub.load('rwightman/pytorch-image-models', 'tf_efficientnetv2_m_in21ft1k')\n",
    "# modelA = modelA.to(device)\n",
    "# modelB = modelB.to(device)\n",
    "#\n",
    "# _acc1, _acc2 = evaluate(test_loader, modelA, modelB)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.hub.list('rwightman/pytorch-image-models')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# import torch\n",
    "#\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# torch.cuda.empty_cache()\n",
    "# modelC = torch.hub.load('rwightman/pytorch-image-models', 'mobilenetv2_100', pretrained=True)\n",
    "# summary(modelC.to(device), input_size=(3, 32, 32))\n",
    "# modelC\n",
    "\n",
    "\n",
    "# from model import ReshapedPreTrainedModel, modify_pretrained_outputs\n",
    "# modelC = ReshapedPreTrainedModel(modelC, n_outputs=10, dropout_rate=0.25, freeze_weights=True)\n",
    "# modelC.to(device)\n",
    "# modelC = modify_pretrained_outputs(modelC)\n",
    "# modelC\n",
    "# modelC = modify_pretrained_outputs(modelC)\n",
    "# modelC.to(device)\n",
    "#\n",
    "# image, label = next(iter(test_loader))\n",
    "# image = image.to(device)\n",
    "# out = modelC(image)\n",
    "#\n",
    "# print(out.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# study = optuna.load_study(study_name=\"cifar10_1\",\n",
    "#                           storage='mysql://root:1115@192.168.50.11:3306/co_mixing')\n",
    "# print(study.best_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# optuna.delete_study(study_name=\"cifar10_Task1_2022_03_17_compare_nomixup\",\n",
    "#                     storage='mysql://root:1115@192.168.50.11:3306/co_mixing')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from optuna.visualization import plot_param_importances\n",
    "#\n",
    "# plot_param_importances(study=study)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}