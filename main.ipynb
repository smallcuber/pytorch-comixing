{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from model import CNN, ReshapedPreTrainedModel\n",
    "from loss import loss_coteaching, loss_coteaching_update\n",
    "from optuna.trial import TrialState\n",
    "import optuna\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mixup import mixup_data\n",
    "from model import dict_models, load_pretrained_model_by_name\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from loader_CIFAR import CifarDataloader, CifarDataset, unpickle\n",
    "from loader_ANIMAL10N import Animal10N\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "\n",
    "# torch.autograd.set_detect_anomaly(True)  this is for debugging\n",
    "\n",
    "name_dataset = 'cifar10'  # either cifar10 or animal10n\n",
    "id_task = 3  # Task 1 to 3, noise file created by TA\n",
    "\n",
    "# path to the data file (don't forget to download the feature data and also put the noisy label file under this folder)\n",
    "if name_dataset == 'animal10n':\n",
    "    data_path = r'rawdata_ANIMAL10N'\n",
    "else:\n",
    "    data_path = r\"C://Users//Wenda//Documents//Pytorch//rawdata_CIFAR10\"\n",
    "\n",
    "num_iter_per_epoch = 100\n",
    "num_print_freq = 100\n",
    "num_epoch = 200\n",
    "\n",
    "if name_dataset == 'animal10n':\n",
    "    num_batch_size = 32  # Have to use a smaller batch for animal10n due to the GPU memory limit (GTX1080 8G)\n",
    "else:\n",
    "    num_batch_size = 128\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    num_batch_size = num_batch_size * torch.cuda.device_count()\n",
    "\n",
    "\n",
    "def tryUsingDataParallel(model):\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using parallel with {torch.cuda.device_count()} GPUs!\")\n",
    "        return torch.nn.DataParallel(model)\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "\n",
    "num_gradual = 10\n",
    "num_exponent = 1\n",
    "num_forget_rate = 0.1\n",
    "num_noise_rate = 0.2\n",
    "num_workers = 6\n",
    "num_classes = 10\n",
    "num_learning_rate = 0.001\n",
    "num_input_channel = 3\n",
    "\n",
    "# Adjust learning rate and betas for Adam Optimizer\n",
    "num_mixup_alpha = 0.1\n",
    "num_epoch_decay_start = 80\n",
    "mom1 = 0.9\n",
    "mom2 = 0.1\n",
    "alpha_plan = [num_learning_rate] * num_epoch\n",
    "beta1_plan = [mom1] * num_epoch\n",
    "for i in range(num_epoch_decay_start, num_epoch):\n",
    "    alpha_plan[i] = float(num_epoch - i) / (num_epoch - num_epoch_decay_start) * num_learning_rate\n",
    "    beta1_plan[i] = mom2\n",
    "\n",
    "rate_schedule = np.ones(num_epoch) * num_forget_rate\n",
    "rate_schedule[:num_gradual] = np.linspace(0, num_forget_rate ** num_exponent, num_gradual)\n",
    "\n",
    "json_noise_file_names = {\n",
    "    1: 'cifar10_noisy_labels_task1.json',\n",
    "    2: 'cifar10_noisy_labels_task2.json',\n",
    "    3: 'cifar10_noisy_labels_task3.json'\n",
    "}\n",
    "noise_file_name = json_noise_file_names[id_task]  # Change The number to switch tasks\n",
    "\n",
    "if name_dataset == 'cifar10':\n",
    "    loader = CifarDataloader(name_dataset, batch_size=128,\n",
    "                             num_workers=10,\n",
    "                             root_dir=data_path,\n",
    "                             noise_file='%s/%s' % (data_path, noise_file_name))\n",
    "    train_loader, noisy_labels, clean_labels = loader.run('train')\n",
    "    noise_or_not = np.transpose(noisy_labels) == np.transpose(clean_labels)\n",
    "    test_loader = loader.run('test')\n",
    "else:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset_train = Animal10N(split='train', data_path=data_path, transform=transform_train)\n",
    "    dataset_test = Animal10N(split='test', data_path=data_path, transform=transform_test)\n",
    "\n",
    "    train_loader = DataLoader(dataset_train, batch_size=num_batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(dataset_test, batch_size=num_batch_size * 2, shuffle=False, num_workers=num_workers)\n",
    "    noise_or_not = None\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if name_dataset == 'animal10n':\n",
    "    model_full_name = f'{name_dataset}'\n",
    "else:\n",
    "    model_full_name = f'{name_dataset}_{noise_file_name}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "#\n",
    "# def imshow(axis, inp):\n",
    "#     \"\"\"Denormalize and show\"\"\"\n",
    "#     inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     mean = np.array([0.485, 0.456, 0.406])\n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     inp = std * inp + mean\n",
    "#     axis.imshow(inp)\n",
    "# img, label, idx = next(iter(train_loader))\n",
    "# print(img.size(), label.size())\n",
    "# fig = plt.figure(1, figsize=(16, 4))\n",
    "# grid = ImageGrid(fig, 111, nrows_ncols=(1, 4), axes_pad=0.05)\n",
    "# for i in range(img.size()[0]):\n",
    "#     ax = grid[i]\n",
    "#     imshow(ax, img[i])\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = alpha_plan[epoch]\n",
    "        param_group['betas'] = (beta1_plan[epoch], 0.999)  # Only change beta1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# TODO: Complete migrating the Co-Teaching model\n",
    "def accuracy(logit, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    output = F.softmax(logit, dim=1)\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def train(loader_train, epoch, model1, optimizer1, model2, optimizer2, criterion, _rate_schedule, _mixup_alpha,\n",
    "          _iter_per_epoch):\n",
    "    print(\"Training... %s\" % model_full_name)\n",
    "    pure_ratio_list = []\n",
    "    _pure_ratio_1_list = []\n",
    "    _pure_ratio_2_list = []\n",
    "    count_total_train1 = 0\n",
    "    count_total_correct1 = 0\n",
    "    count_total_train2 = 0\n",
    "    count_total_correct2 = 0\n",
    "    num_correct_1 = 0\n",
    "    num_correct_2 = 0\n",
    "    num_total = 0\n",
    "    model1.to(device)\n",
    "    model2.to(device)\n",
    "\n",
    "    for i, (images, labels, indexes) in enumerate(loader_train):\n",
    "        # ind = indexes.cpu().numpy().transpose()\n",
    "        ind = indexes.numpy().transpose()\n",
    "        if i > _iter_per_epoch > 0:\n",
    "            break\n",
    "\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        num_total = labels.size(0)\n",
    "\n",
    "        images, label_a, label_b, lam = mixup_data(images, labels, device, _mixup_alpha)\n",
    "\n",
    "        # Forward Backward Optimize\n",
    "        output1 = model1(images)\n",
    "        _, predicted1 = torch.max(output1.data, 1)\n",
    "        prec1, _ = accuracy(output1, labels, topk=(1, 5))\n",
    "        count_total_train1 += 1\n",
    "        count_total_correct1 += prec1\n",
    "\n",
    "        output2 = model2(images)\n",
    "        _, predicted2 = torch.max(output2.data, 1)\n",
    "        prec2, _ = accuracy(output2, labels, topk=(1, 5))\n",
    "        count_total_train2 += 1\n",
    "        count_total_correct2 += prec2\n",
    "\n",
    "        num_correct_1 += (lam * predicted1.eq(label_a.data).cpu().sum().float()\n",
    "                          + (1 - lam) * predicted1.eq(label_b.data).cpu().sum().float())\n",
    "        num_correct_2 += (lam * predicted2.eq(label_a.data).cpu().sum().float()\n",
    "                          + (1 - lam) * predicted2.eq(label_b.data).cpu().sum().float())\n",
    "        # num_correct_1 += (lam * predicted1.eq(label_a.data).cpu().sum().float()\n",
    "        #                   + (1 - lam) * predicted1.eq(label_b.data).cpu().sum().float())\n",
    "        # num_correct_2 += (lam * predicted2.eq(label_a.data).cpu().sum().float()\n",
    "        #                   + (1 - lam) * predicted2.eq(label_b.data).cpu().sum().float())\n",
    "        num_acc_1 = num_correct_1 / num_total\n",
    "        num_acc_2 = num_correct_2 / num_total\n",
    "\n",
    "        loss1, loss2, pure_ratio_1, pure_ratio_2 = loss_coteaching(criterion, output1, output2, labels, label_a,\n",
    "                                                                   label_b, _rate_schedule[epoch], ind, noise_or_not,\n",
    "                                                                   lam)\n",
    "\n",
    "        if pure_ratio_1 and pure_ratio_2 is not None:\n",
    "            _pure_ratio_1_list.append(100 * pure_ratio_1)\n",
    "            _pure_ratio_2_list.append(100 * pure_ratio_2)\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        if (i + 1) % num_print_freq == 0:\n",
    "            if pure_ratio_1 and pure_ratio_2 is not None:\n",
    "                str_calc_pure_ratio = 'Pure Ratio1: %.4f, Pure Ratio2 %.4f' % (\n",
    "                    np.sum(_pure_ratio_1_list) / len(_pure_ratio_1_list),\n",
    "                    np.sum(_pure_ratio_2_list) / len(_pure_ratio_2_list))\n",
    "            else:\n",
    "                str_calc_pure_ratio = 'Animal10N dataset without pure ratio'\n",
    "\n",
    "            print(\n",
    "                'Train Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f, %s'\n",
    "                % (epoch + 1, num_epoch, i + 1, len(train_loader) // num_batch_size, num_acc_1, num_acc_2, loss1.item(),\n",
    "                   loss2.item(), str_calc_pure_ratio))\n",
    "\n",
    "    train_acc1 = float(count_total_correct1) / float(count_total_train1)\n",
    "    train_acc2 = float(count_total_correct2) / float(count_total_train2)\n",
    "    return train_acc1, train_acc2, _pure_ratio_1_list, _pure_ratio_2_list\n",
    "\n",
    "\n",
    "def train_improve(loader_train, epoch, model1, optimizer1, model2, optimizer2, criterion, _rate_schedule, _mixup_alpha,\n",
    "          _iter_per_epoch):\n",
    "    print(\"Training... %s\" % model_full_name)\n",
    "    pure_ratio_list = []\n",
    "    _pure_ratio_1_list = []\n",
    "    _pure_ratio_2_list = []\n",
    "    count_total_train1 = 0\n",
    "    count_total_correct1 = 0\n",
    "    count_total_train2 = 0\n",
    "    count_total_correct2 = 0\n",
    "    num_correct_1 = 0\n",
    "    num_correct_2 = 0\n",
    "    num_acc_1 = 0\n",
    "    num_acc_2 = 0\n",
    "    num_total = 0\n",
    "    num_total_1 = 0\n",
    "    num_total_2 = 0\n",
    "    model1.to(device)\n",
    "    model2.to(device)\n",
    "\n",
    "    for i, (images, labels, indexes) in enumerate(loader_train):\n",
    "        # ind = indexes.cpu().numpy().transpose()\n",
    "        ind = indexes.numpy().transpose()\n",
    "        if i > _iter_per_epoch > 0:\n",
    "            break\n",
    "\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        # num_total = labels.size(0)\n",
    "\n",
    "        # Forward Backward Optimize\n",
    "        output1 = model1(images)\n",
    "        output2 = model2(images)\n",
    "\n",
    "        loss1, loss2, pure_ratio_1, pure_ratio_2,\\\n",
    "        lam_2, lam_1, \\\n",
    "        images_1, images_2,\\\n",
    "        label_1_a, label_1_b,\\\n",
    "        label_2_a, label_2_b,\\\n",
    "            output1_mix, output2_mix = loss_coteaching_update(criterion,\n",
    "                                                      model1, model2,\n",
    "                                                      output1, output2,\n",
    "                                                      images,\n",
    "                                                      labels,\n",
    "                                                      _rate_schedule[epoch],\n",
    "                                                      ind,\n",
    "                                                      noise_or_not,\n",
    "                                                      _mixup_alpha, device)\n",
    "\n",
    "        num_total_1 += label_2_a.size(0)\n",
    "        num_total_2 += label_1_a.size(0)\n",
    "\n",
    "        # Forward Backward Optimize\n",
    "        _, predicted1 = torch.max(output1_mix.data, 1)\n",
    "        _, predicted2 = torch.max(output2_mix.data, 1)\n",
    "\n",
    "        num_correct_1 += (lam_2 * predicted1.eq(label_2_a.data).cpu().sum().float()\n",
    "                          + (1 - lam_2) * predicted1.eq(label_2_b.data).cpu().sum().float())\n",
    "        num_correct_2 += (lam_1 * predicted2.eq(label_1_a.data).cpu().sum().float()\n",
    "                          + (1 - lam_1) * predicted2.eq(label_1_b.data).cpu().sum().float())\n",
    "        num_acc_1 = num_correct_1 / num_total_1\n",
    "        num_acc_2 = num_correct_2 / num_total_2\n",
    "\n",
    "        if pure_ratio_1 and pure_ratio_2 is not None:\n",
    "            _pure_ratio_1_list.append(100 * pure_ratio_1)\n",
    "            _pure_ratio_2_list.append(100 * pure_ratio_2)\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        if (i + 1) % num_print_freq == 0:\n",
    "            if pure_ratio_1 and pure_ratio_2 is not None:\n",
    "                str_calc_pure_ratio = 'Pure Ratio1: %.4f, Pure Ratio2 %.4f' % (\n",
    "                    np.sum(_pure_ratio_1_list) / len(_pure_ratio_1_list),\n",
    "                    np.sum(_pure_ratio_2_list) / len(_pure_ratio_2_list))\n",
    "            else:\n",
    "                str_calc_pure_ratio = 'Animal10N dataset without pure ratio'\n",
    "\n",
    "            print(\n",
    "                'Train Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F (%d/%d), Training Accuracy2: %.4f (%d/%d), Loss1: %.4f, Loss2: %.4f, %s'\n",
    "                % (epoch + 1, num_epoch, i + 1, len(train_loader) // num_batch_size, num_acc_1, num_correct_1, num_total_1, num_acc_2, num_correct_2, num_total_2, loss1.item(),\n",
    "                   loss2.item(), str_calc_pure_ratio))\n",
    "\n",
    "    return num_acc_1, num_acc_2, _pure_ratio_1_list, _pure_ratio_2_list\n",
    "\n",
    "\n",
    "def evaluate(_test_loader, model1, model2, particial_test_rate=0):\n",
    "    print('Evaluating %s...' % model_full_name)\n",
    "    # model1 = model1.to(device)\n",
    "    # model2 = model2.to(device)\n",
    "    model1.eval()  # Change model to 'eval' mode.\n",
    "    correct1 = 0\n",
    "    total1 = 0\n",
    "    print(\"Start evaluating model 1\")\n",
    "    for images, labels in _test_loader:\n",
    "        images = Variable(images).to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits1 = model1(images)\n",
    "        outputs1 = F.softmax(logits1, dim=1)\n",
    "        _, pred1 = torch.max(outputs1.data, 1)\n",
    "        total1 += labels.size(0)\n",
    "        correct1 += (pred1 == labels).sum()\n",
    "\n",
    "    print(\"Start evaluating model 2\")\n",
    "    model2.eval()  # Change model to 'eval' mode\n",
    "    correct2 = 0\n",
    "    total2 = 0\n",
    "    for images, labels in _test_loader:\n",
    "        images = Variable(images).to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits2 = model2(images)\n",
    "        outputs2 = F.softmax(logits2, dim=1)\n",
    "        _, pred2 = torch.max(outputs2.data, 1)\n",
    "        total2 += labels.size(0)\n",
    "        correct2 += (pred2 == labels).sum()\n",
    "\n",
    "    acc1 = 100 * float(correct1) / float(total1)\n",
    "    acc2 = 100 * float(correct2) / float(total2)\n",
    "    return acc1, acc2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "\n",
    "def main_single_train():\n",
    "    # Train the model\n",
    "\n",
    "    # Data Loader (Input Pipeline)\n",
    "    print('loading dataset...')\n",
    "\n",
    "    # Define models\n",
    "    print('building model...')\n",
    "    cnn1 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "    # cnn1 = ReshapedPreTrainedModel(modelC, n_outputs=10, dropout_rate=0.25, freeze_weights=True)\n",
    "    cnn1 = tryUsingDataParallel(cnn1)\n",
    "    cnn1.to(device)\n",
    "    # print(cnn1.parameters)\n",
    "    optimizer1 = torch.optim.Adam(cnn1.parameters(), lr=num_learning_rate)\n",
    "\n",
    "    cnn2 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "    # cnn2 = ReshapedPreTrainedModel(modelC, n_outputs=10, dropout_rate=0.25, freeze_weights=True)\n",
    "    cnn2 = tryUsingDataParallel(cnn2)\n",
    "    cnn2.to(device)\n",
    "    # print(cnn2.parameters)\n",
    "    optimizer2 = torch.optim.Adam(cnn2.parameters(), lr=num_learning_rate)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduce=False)\n",
    "\n",
    "    # training\n",
    "    for epoch in range(1, num_epoch):\n",
    "        # train models\n",
    "        cnn1.train()\n",
    "        adjust_learning_rate(optimizer1, epoch)\n",
    "        cnn2.train()\n",
    "        adjust_learning_rate(optimizer2, epoch)\n",
    "        train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list = train(train_loader, epoch, cnn1, optimizer1,\n",
    "                                                                             cnn2, optimizer2, criterion, rate_schedule,\n",
    "                                                                             num_mixup_alpha, num_iter_per_epoch)\n",
    "        # evaluate models\n",
    "        test_acc1, test_acc2 = evaluate(test_loader, cnn1, cnn2)\n",
    "        # save results\n",
    "        if name_dataset == 'cifar10n':\n",
    "            mean_pure_ratio1 = sum(pure_ratio_1_list) / len(pure_ratio_1_list)\n",
    "            mean_pure_ratio2 = sum(pure_ratio_2_list) / len(pure_ratio_2_list)\n",
    "            str_ratio = 'Pure Ratio 1 %.4f %%, Pure Ratio 2 %.4f' % (mean_pure_ratio1, mean_pure_ratio2)\n",
    "        else:\n",
    "            str_ratio = 'animal10n without pure ratio.'\n",
    "        print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %%, %s' % (\n",
    "            epoch + 1, num_epoch, len(test_loader.dataset), test_acc1, test_acc2, str_ratio))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def train_trial(_model_select,\n",
    "                _optimizer_select,\n",
    "                _learning_rate_scheduler_select,\n",
    "                trial,\n",
    "                _forget_rate=0.1, _gradual=10, _num_epoch=200, _num_mixup_alpha=0.1, _iter_per_epoch=200,\n",
    "                _num_exponent=1, _learning_rate=0.02, _freq_evaluate=1, _freeze_weights=True, _train_improve=False):\n",
    "    _rate_schedule = np.ones(_num_epoch) * _forget_rate\n",
    "    _rate_schedule[:_gradual] = np.linspace(0, _forget_rate ** _num_exponent, _gradual)\n",
    "\n",
    "    print('Start trail...')\n",
    "    # Define models\n",
    "    print('building model...')\n",
    "    if _model_select != \"CNN\":\n",
    "        print(f'Loading pre-trained model: {_model_select}')\n",
    "        _model1 = load_pretrained_model_by_name(_model_select, is_pretrained=True)\n",
    "        _model2 = load_pretrained_model_by_name(_model_select, is_pretrained=True)\n",
    "\n",
    "        if _freeze_weights:\n",
    "            for param in _model1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in _model2.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_input = _model1.classifier.in_features\n",
    "        _model1.classifier = torch.nn.Linear(num_input, num_classes, bias=True).to(device)\n",
    "        num_input = _model2.classifier.in_features\n",
    "        _model2.classifier = torch.nn.Linear(num_input, num_classes, bias=True).to(device)\n",
    "        # _model1 = modify_pretrained_outputs(_model1.to(device), num_output=num_classes, freeze_parameters=_freeze_weights)\n",
    "        # _model1 = modify_pretrained_outputs(_model2.to(device), num_output=num_classes, freeze_parameters=_freeze_weights)\n",
    "    else:\n",
    "        _model1 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "        _model2 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "\n",
    "    _model1 = tryUsingDataParallel(_model1)\n",
    "    _model2 = tryUsingDataParallel(_model2)\n",
    "    _model1.to(device)\n",
    "    _model2.to(device)\n",
    "\n",
    "    if _optimizer_select == \"Adam\":\n",
    "        _optm1 = torch.optim.Adam(_model1.parameters(), lr=_learning_rate)\n",
    "        _optm2 = torch.optim.Adam(_model2.parameters(), lr=_learning_rate)\n",
    "    else:\n",
    "        _optm1 = torch.optim.SGD(_model1.parameters(), lr=_learning_rate)\n",
    "        _optm2 = torch.optim.SGD(_model2.parameters(), lr=_learning_rate)\n",
    "\n",
    "    if _learning_rate_scheduler_select == \"StepLR\":\n",
    "        scheduler1 = lr_scheduler.StepLR(_optm1, step_size=30, gamma=0.5)\n",
    "        scheduler2 = lr_scheduler.StepLR(_optm2, step_size=30, gamma=0.5)\n",
    "\n",
    "    _criterion_select = torch.nn.CrossEntropyLoss(reduce=False)\n",
    "    _mean_pure_ratio1 = 0\n",
    "    _mean_pure_ratio2 = 0\n",
    "    _epoch = 0\n",
    "    _train_acc1 = 0\n",
    "    _train_acc2 = 0\n",
    "    _test_acc1, _test_acc2 = 0, 0\n",
    "\n",
    "    print(\n",
    "        f\"Current model: {_model_select}, current optimizer: {_optimizer_select}, criterion: {_criterion_select}, lr schedule: {_learning_rate_scheduler_select}, forget rate: {_forget_rate}, gradual rate: {_gradual}, mixup alpha: {_num_mixup_alpha}\")\n",
    "\n",
    "    for _epoch in range(_num_epoch):\n",
    "        # train models\n",
    "        _model1.train()\n",
    "        _model2.train()\n",
    "\n",
    "        if _train_improve:\n",
    "            _train_acc1, _train_acc2, _pure_ratio_1_list, _pure_ratio_2_list = train_improve(train_loader, _epoch, _model1, _optm1,\n",
    "                                                                                     _model2, _optm2, _criterion_select,\n",
    "                                                                                     _rate_schedule, _num_mixup_alpha,\n",
    "                                                                                     _iter_per_epoch)\n",
    "        else:\n",
    "            _train_acc1, _train_acc2, _pure_ratio_1_list, _pure_ratio_2_list = train(train_loader, _epoch, _model1, _optm1,\n",
    "                                                                                     _model2, _optm2, _criterion_select,\n",
    "                                                                                     _rate_schedule, _num_mixup_alpha,\n",
    "                                                                                     _iter_per_epoch)\n",
    "\n",
    "        if _learning_rate_scheduler_select == \"StepLR\":\n",
    "            scheduler1.step(_epoch)\n",
    "            scheduler2.step(_epoch)\n",
    "        else:\n",
    "            adjust_learning_rate(_optm1, _epoch)\n",
    "            adjust_learning_rate(_optm2, _epoch)\n",
    "\n",
    "        if (_epoch + 1) % _freq_evaluate == 0:\n",
    "            # evaluate models\n",
    "            _test_acc1, _test_acc2 = evaluate(test_loader, _model1, _model2)\n",
    "            # save results\n",
    "            if name_dataset == 'cifar10':\n",
    "                _mean_pure_ratio1 = sum(_pure_ratio_1_list) / len(_pure_ratio_1_list)\n",
    "                _mean_pure_ratio2 = sum(_pure_ratio_2_list) / len(_pure_ratio_2_list)\n",
    "                _str_ratio = 'Pure Ratio 1 %.4f %%, Pure Ratio 2 %.4f' % (_mean_pure_ratio1, _mean_pure_ratio2)\n",
    "            else:\n",
    "                _str_ratio = 'animal10n without pure ratio.'\n",
    "            print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %%, %s' % (\n",
    "                _epoch + 1, num_epoch, len(test_loader.dataset), _test_acc1, _test_acc2, _str_ratio))\n",
    "\n",
    "            _temp_acc = max(_test_acc1, _test_acc2)  # current best testing accuracy\n",
    "            if trial is not None:\n",
    "                trial.report(_temp_acc, _epoch)\n",
    "                # TODO: Renable this when checking default performance is done\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "                else:\n",
    "                    print(\"Trial not pruned yet\")\n",
    "            else:\n",
    "                print(\"Trial input is none, assume the study is a single test run\")\n",
    "\n",
    "    return max(_test_acc1, _test_acc2)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    #  _criterion_select = trial.suggest_categorical(\n",
    "    #      \"Criterion\",\n",
    "    #      [\"CEL\"])\n",
    "    _forget_rate_selection = trial.suggest_categorical(\n",
    "        \"Forget Rate\",\n",
    "        [0.05, 0.1, 0.2, 0.5])\n",
    "    _gradual = trial.suggest_categorical(\n",
    "        \"Gradual\",\n",
    "        [10, 50])\n",
    "    _mixup_alpha = trial.suggest_categorical(\n",
    "        \"Mixup Alpha\",\n",
    "        [0, 0.05, 0.1, 0.5])\n",
    "    _optm_select = trial.suggest_categorical(\n",
    "        \"Optimizer\",\n",
    "        [\"SGD\", \"Adam\"])\n",
    "    _lr_scheduler_select = trial.suggest_categorical(\n",
    "        \"Scheduler\",\n",
    "        [\"StepLR\", \"None\"]\n",
    "    )\n",
    "    _lr_select = trial.suggest_categorical(\n",
    "        \"Learning Rate\",\n",
    "        [0.08, 0.04, 0.02, 0.008, 0.004]\n",
    "    )\n",
    "    _model_select = trial.suggest_categorical(  # TODO: Search models for training\n",
    "        \"Model\",\n",
    "        [\"CNN\",\n",
    "         \"Densenet121\",\n",
    "         \"EfficientNet_v2_rw_t\", \"EfficientNet_v2_s\", \"EfficientNet_v2_m\", \"Mobilenet_v2_035\", \"Mobilenet_v2_100\"]\n",
    "    )\n",
    "\n",
    "    _accuracy = train_trial(_model_select,\n",
    "                            _optm_select,\n",
    "                            _lr_scheduler_select,\n",
    "                            trial=trial,\n",
    "                            _forget_rate=_forget_rate_selection, _gradual=_gradual, _num_epoch=200,\n",
    "                            _num_mixup_alpha=_mixup_alpha, _iter_per_epoch=200, _num_exponent=1, _learning_rate=0.04,\n",
    "                            _freq_evaluate=5, _freeze_weights=False, _train_improve=True)\n",
    "    return _accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run the study here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-04-17 15:06:02,623]\u001B[0m A new study created in RDB with name: cifar10_Task3_2022_04_17_alg_improve\u001B[0m\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trail...\n",
      "building model...\n",
      "Current model: CNN, current optimizer: SGD, criterion: CrossEntropyLoss(), lr schedule: StepLR, forget rate: 0.1, gradual rate: 50, mixup alpha: 0.1\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [1/200], Iter [100/3] Training Accuracy1: 0.2205 (2821/12800), Training Accuracy2: 0.2226 (2849/12800), Loss1: 1.9044, Loss2: 1.7602, Pure Ratio1: 80.0703, Pure Ratio2 80.0703\n",
      "Train Epoch [1/200], Iter [200/3] Training Accuracy1: 0.2372 (6071/25600), Training Accuracy2: 0.2411 (6172/25600), Loss1: 1.4546, Loss2: 1.4617, Pure Ratio1: 80.0156, Pure Ratio2 80.0156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [2/200], Iter [100/3] Training Accuracy1: 0.2909 (3693/12700), Training Accuracy2: 0.2904 (3688/12700), Loss1: 1.4096, Loss2: 1.3854, Pure Ratio1: 80.3386, Pure Ratio2 80.3465\n",
      "Train Epoch [2/200], Iter [200/3] Training Accuracy1: 0.2919 (7415/25400), Training Accuracy2: 0.2963 (7525/25400), Loss1: 1.4775, Loss2: 1.5674, Pure Ratio1: 80.3701, Pure Ratio2 80.3661\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [3/200], Iter [100/3] Training Accuracy1: 0.2928 (3718/12700), Training Accuracy2: 0.2980 (3784/12700), Loss1: 1.9855, Loss2: 1.3066, Pure Ratio1: 79.9291, Pure Ratio2 79.9449\n",
      "Train Epoch [3/200], Iter [200/3] Training Accuracy1: 0.3019 (7667/25400), Training Accuracy2: 0.3076 (7811/25400), Loss1: 1.1842, Loss2: 1.1297, Pure Ratio1: 80.1024, Pure Ratio2 80.0748\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [4/200], Iter [100/3] Training Accuracy1: 0.2765 (3512/12700), Training Accuracy2: 0.2797 (3551/12700), Loss1: 1.0279, Loss2: 1.0247, Pure Ratio1: 79.9213, Pure Ratio2 79.8819\n",
      "Train Epoch [4/200], Iter [200/3] Training Accuracy1: 0.3233 (8212/25400), Training Accuracy2: 0.3244 (8239/25400), Loss1: 1.0890, Loss2: 1.0783, Pure Ratio1: 80.2717, Pure Ratio2 80.2362\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [5/200], Iter [100/3] Training Accuracy1: 0.3311 (4171/12600), Training Accuracy2: 0.3329 (4193/12600), Loss1: 1.0741, Loss2: 1.9159, Pure Ratio1: 79.9683, Pure Ratio2 79.8889\n",
      "Train Epoch [5/200], Iter [200/3] Training Accuracy1: 0.3284 (8275/25200), Training Accuracy2: 0.3289 (8289/25200), Loss1: 1.0854, Loss2: 0.8497, Pure Ratio1: 80.1508, Pure Ratio2 80.1310\n",
      "Evaluating cifar10_cifar10_noisy_labels_task3.json...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [5/200] Test Accuracy on the 10000 test images: Model1 60.1600 % Model2 60.5900 %, Pure Ratio 1 80.1390 %, Pure Ratio 2 80.1192\n",
      "Trial not pruned yet\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [6/200], Iter [100/3] Training Accuracy1: 0.3664 (4616/12600), Training Accuracy2: 0.3711 (4675/12600), Loss1: 1.1462, Loss2: 1.0307, Pure Ratio1: 80.9683, Pure Ratio2 80.9762\n",
      "Train Epoch [6/200], Iter [200/3] Training Accuracy1: 0.3703 (9330/25200), Training Accuracy2: 0.3723 (9382/25200), Loss1: 1.0732, Loss2: 1.8165, Pure Ratio1: 80.4405, Pure Ratio2 80.4365\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [7/200], Iter [100/3] Training Accuracy1: 0.3537 (4456/12600), Training Accuracy2: 0.3542 (4462/12600), Loss1: 0.9982, Loss2: 0.9700, Pure Ratio1: 80.1349, Pure Ratio2 80.0556\n",
      "Train Epoch [7/200], Iter [200/3] Training Accuracy1: 0.3654 (9208/25200), Training Accuracy2: 0.3672 (9253/25200), Loss1: 0.9249, Loss2: 0.9039, Pure Ratio1: 80.3254, Pure Ratio2 80.2302\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [8/200], Iter [100/3] Training Accuracy1: 0.3733 (4703/12600), Training Accuracy2: 0.3769 (4748/12600), Loss1: 1.6327, Loss2: 1.0212, Pure Ratio1: 80.0873, Pure Ratio2 80.1270\n",
      "Train Epoch [8/200], Iter [200/3] Training Accuracy1: 0.3732 (9404/25200), Training Accuracy2: 0.3761 (9477/25200), Loss1: 1.5817, Loss2: 1.0413, Pure Ratio1: 80.2063, Pure Ratio2 80.2063\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [9/200], Iter [100/3] Training Accuracy1: 0.3558 (4447/12500), Training Accuracy2: 0.3567 (4458/12500), Loss1: 0.9683, Loss2: 1.0575, Pure Ratio1: 80.8240, Pure Ratio2 80.8480\n",
      "Train Epoch [9/200], Iter [200/3] Training Accuracy1: 0.3762 (9405/25000), Training Accuracy2: 0.3771 (9426/25000), Loss1: 1.0481, Loss2: 0.9084, Pure Ratio1: 80.4880, Pure Ratio2 80.4840\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [10/200], Iter [100/3] Training Accuracy1: 0.3843 (4803/12500), Training Accuracy2: 0.3822 (4777/12500), Loss1: 0.9126, Loss2: 0.9599, Pure Ratio1: 80.6160, Pure Ratio2 80.5360\n",
      "Train Epoch [10/200], Iter [200/3] Training Accuracy1: 0.3780 (9450/25000), Training Accuracy2: 0.3739 (9346/25000), Loss1: 1.7807, Loss2: 0.9538, Pure Ratio1: 80.3880, Pure Ratio2 80.3360\n",
      "Evaluating cifar10_cifar10_noisy_labels_task3.json...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [10/200] Test Accuracy on the 10000 test images: Model1 68.1000 % Model2 68.1100 %, Pure Ratio 1 80.4060 %, Pure Ratio 2 80.3582\n",
      "Trial not pruned yet\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [11/200], Iter [100/3] Training Accuracy1: 0.3878 (4848/12500), Training Accuracy2: 0.3777 (4721/12500), Loss1: 0.8144, Loss2: 0.8638, Pure Ratio1: 80.7280, Pure Ratio2 80.7360\n",
      "Train Epoch [11/200], Iter [200/3] Training Accuracy1: 0.3844 (9610/25000), Training Accuracy2: 0.3803 (9508/25000), Loss1: 0.9211, Loss2: 0.9490, Pure Ratio1: 80.4000, Pure Ratio2 80.3600\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [12/200], Iter [100/3] Training Accuracy1: 0.4037 (5046/12500), Training Accuracy2: 0.4132 (5164/12500), Loss1: 0.7416, Loss2: 1.1884, Pure Ratio1: 80.0800, Pure Ratio2 79.9920\n",
      "Train Epoch [12/200], Iter [200/3] Training Accuracy1: 0.3865 (9662/25000), Training Accuracy2: 0.3911 (9776/25000), Loss1: 0.8279, Loss2: 0.9568, Pure Ratio1: 80.3920, Pure Ratio2 80.3880\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [13/200], Iter [100/3] Training Accuracy1: 0.3685 (4568/12400), Training Accuracy2: 0.3824 (4741/12400), Loss1: 0.8582, Loss2: 0.9032, Pure Ratio1: 80.8468, Pure Ratio2 80.9758\n",
      "Train Epoch [13/200], Iter [200/3] Training Accuracy1: 0.3773 (9357/24800), Training Accuracy2: 0.3847 (9539/24800), Loss1: 0.7790, Loss2: 0.8207, Pure Ratio1: 80.6452, Pure Ratio2 80.7823\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [14/200], Iter [100/3] Training Accuracy1: 0.3675 (4556/12400), Training Accuracy2: 0.3621 (4490/12400), Loss1: 0.8484, Loss2: 0.7647, Pure Ratio1: 80.2177, Pure Ratio2 80.1371\n",
      "Train Epoch [14/200], Iter [200/3] Training Accuracy1: 0.3668 (9095/24800), Training Accuracy2: 0.3661 (9079/24800), Loss1: 0.7314, Loss2: 0.8122, Pure Ratio1: 80.0887, Pure Ratio2 80.0202\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [15/200], Iter [100/3] Training Accuracy1: 0.4210 (5220/12400), Training Accuracy2: 0.4196 (5203/12400), Loss1: 1.9030, Loss2: 0.8413, Pure Ratio1: 80.3468, Pure Ratio2 80.3871\n",
      "Train Epoch [15/200], Iter [200/3] Training Accuracy1: 0.3796 (9413/24800), Training Accuracy2: 0.3792 (9404/24800), Loss1: 1.6797, Loss2: 0.8051, Pure Ratio1: 80.1532, Pure Ratio2 80.1734\n",
      "Evaluating cifar10_cifar10_noisy_labels_task3.json...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [15/200] Test Accuracy on the 10000 test images: Model1 73.6800 % Model2 74.5100 %, Pure Ratio 1 80.1797 %, Pure Ratio 2 80.1998\n",
      "Trial not pruned yet\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [16/200], Iter [100/3] Training Accuracy1: 0.3703 (4592/12400), Training Accuracy2: 0.3639 (4511/12400), Loss1: 0.6156, Loss2: 0.6472, Pure Ratio1: 80.4597, Pure Ratio2 80.4194\n",
      "Train Epoch [16/200], Iter [200/3] Training Accuracy1: 0.3849 (9544/24800), Training Accuracy2: 0.3845 (9536/24800), Loss1: 0.7037, Loss2: 0.8225, Pure Ratio1: 80.2177, Pure Ratio2 80.1815\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [17/200], Iter [100/3] Training Accuracy1: 0.3925 (4827/12300), Training Accuracy2: 0.3970 (4883/12300), Loss1: 0.9120, Loss2: 0.7540, Pure Ratio1: 80.3089, Pure Ratio2 80.3171\n",
      "Train Epoch [17/200], Iter [200/3] Training Accuracy1: 0.3895 (9582/24600), Training Accuracy2: 0.3952 (9721/24600), Loss1: 0.6612, Loss2: 0.6613, Pure Ratio1: 80.6667, Pure Ratio2 80.6707\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [18/200], Iter [100/3] Training Accuracy1: 0.3783 (4652/12300), Training Accuracy2: 0.3902 (4798/12300), Loss1: 0.6458, Loss2: 0.6023, Pure Ratio1: 80.4472, Pure Ratio2 80.3821\n",
      "Train Epoch [18/200], Iter [200/3] Training Accuracy1: 0.4164 (10243/24600), Training Accuracy2: 0.4248 (10449/24600), Loss1: 0.6258, Loss2: 0.6162, Pure Ratio1: 80.5244, Pure Ratio2 80.4837\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [19/200], Iter [100/3] Training Accuracy1: 0.4093 (5034/12300), Training Accuracy2: 0.4068 (5004/12300), Loss1: 1.2506, Loss2: 0.6367, Pure Ratio1: 80.3740, Pure Ratio2 80.4309\n",
      "Train Epoch [19/200], Iter [200/3] Training Accuracy1: 0.3828 (9416/24600), Training Accuracy2: 0.3762 (9255/24600), Loss1: 1.0839, Loss2: 0.9087, Pure Ratio1: 80.5203, Pure Ratio2 80.4959\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [20/200], Iter [100/3] Training Accuracy1: 0.3951 (4859/12300), Training Accuracy2: 0.4004 (4924/12300), Loss1: 1.1715, Loss2: 0.9260, Pure Ratio1: 80.1382, Pure Ratio2 80.1707\n",
      "Train Epoch [20/200], Iter [200/3] Training Accuracy1: 0.4176 (10272/24600), Training Accuracy2: 0.4228 (10400/24600), Loss1: 0.6624, Loss2: 1.7403, Pure Ratio1: 80.1992, Pure Ratio2 80.2480\n",
      "Evaluating cifar10_cifar10_noisy_labels_task3.json...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [20/200] Test Accuracy on the 10000 test images: Model1 72.4000 % Model2 74.2600 %, Pure Ratio 1 80.1925 %, Pure Ratio 2 80.2330\n",
      "Trial not pruned yet\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n",
      "Train Epoch [21/200], Iter [100/3] Training Accuracy1: 0.4081 (4978/12200), Training Accuracy2: 0.4046 (4936/12200), Loss1: 0.5962, Loss2: 0.6928, Pure Ratio1: 81.3525, Pure Ratio2 81.2541\n",
      "Train Epoch [21/200], Iter [200/3] Training Accuracy1: 0.3939 (9612/24400), Training Accuracy2: 0.3913 (9548/24400), Loss1: 0.6772, Loss2: 0.9167, Pure Ratio1: 80.7049, Pure Ratio2 80.7377\n",
      "Training... cifar10_cifar10_noisy_labels_task3.json\n"
     ]
    }
   ],
   "source": [
    "study_trial = True\n",
    "\n",
    "if name_dataset == \"cifar10\":\n",
    "    study_name = f\"{name_dataset}_Task{id_task}_2022_04_17_alg_improve\"  # update the date manually here\n",
    "else:  # When the dataset is animal10n\n",
    "    study_name = f\"{name_dataset}_unfreeze_weight_nomixup\"\n",
    "\n",
    "# search_space = {\"Forget Rate\":[0.05, 0.1, 0.2, 0.5],\n",
    "#                 \"Gradual\":[10, 50],\n",
    "#                 \"Mixup Alpha\":[0, 0.05, 0.1, 0.5],\n",
    "#                 \"Optimizer\":[\"SGD\", \"Adam\"],\n",
    "#                 \"Scheduler\":[\"StepLR\", \"None\"],\n",
    "#                 \"Learning Rate\":[0.08, 0.04, 0.02, 0.008, 0.004],\n",
    "#                 \"Model\":[\"CNN\",\"Densenet121\",\"EfficientNet_v2_rw_t\"]\n",
    "#                 }\n",
    "# search_space = {\"Forget Rate\":[0.05, 0.1, 0.2, 0.5],\n",
    "#                 \"Gradual\":[10, 50],\n",
    "#                 \"Mixup Alpha\":[0, 0.05, 0.1, 0.5],\n",
    "#                 \"Optimizer\":[\"SGD\"],\n",
    "#                 \"Scheduler\":[\"StepLR\"],\n",
    "#                 \"Learning Rate\":[0.08, 0.04, 0.02, 0.008, 0.004],\n",
    "#                 \"Model\":[\"CNN\",\"Densenet121\",\"EfficientNet_v2_rw_t\"]\n",
    "#                 }\n",
    "\n",
    "\n",
    "# if study_trial:\n",
    "#     search_space = {\"Forget Rate\":[0.05, 0.1, 0.2, 0.5],\n",
    "#                     \"Gradual\":[10, 50],\n",
    "#                     \"Mixup Alpha\":[0, 0.05, 0.1, 0.5],\n",
    "#                     \"Optimizer\":[\"SGD\"],\n",
    "#                     \"Scheduler\":[\"StepLR\"],\n",
    "#                     \"Learning Rate\":[0.08, 0.04, 0.02, 0.008, 0.004],\n",
    "#                     \"Model\":[\"EfficientNet_v2_s\", \"EfficientNet_v2_m\", \"Mobilenet_v2_035\", \"Mobilenet_v2_100\"]\n",
    "#                     }\n",
    "\n",
    "# if study_trial:\n",
    "#     search_space = {\"Forget Rate\": [0.2, 0.5],\n",
    "#                     \"Gradual\": [50],\n",
    "#                     \"Mixup Alpha\": [0],\n",
    "#                     \"Optimizer\": [\"SGD\"],\n",
    "#                     \"Scheduler\": [\"StepLR\"],\n",
    "#                     \"Learning Rate\": [0.08, 0.04],\n",
    "#                     \"Model\": [\"EfficientNet_v2_m\"]\n",
    "#                     }\n",
    "\n",
    "if study_trial:\n",
    "    search_space = {\"Forget Rate\":[0.1, 0.2, 0.5, 0.7],\n",
    "                    \"Gradual\":[50],\n",
    "                    \"Mixup Alpha\":[0.05, 0.1, 0.5],\n",
    "                    \"Optimizer\":[\"SGD\"],\n",
    "                    \"Scheduler\":[\"StepLR\"],\n",
    "                    \"Learning Rate\":[0.08, 0.04, 0.02, 0.008],\n",
    "                    \"Model\":[\"CNN\"]\n",
    "                    }\n",
    "\n",
    "    # TODO: Configure MySQL DB for remote connection\n",
    "    study = optuna.create_study(study_name=study_name,\n",
    "                                storage='mysql://root:1115@192.168.50.11:3306/co_mixing',\n",
    "                                load_if_exists=True,\n",
    "                                direction=\"maximize\",\n",
    "                                sampler=optuna.samplers.GridSampler(search_space))\n",
    "\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Facts:\")\n",
    "    print(\"  Finished Trials: \", len(study.trials))\n",
    "    print(\"  Pruned Trials: \", len(pruned_trials))\n",
    "    print(\"  Complete Trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best Trial:\")\n",
    "    trial_best = study.best_trial\n",
    "    print(\"  Value: \", trial_best.value)\n",
    "    print(\"  Parameters: \")\n",
    "    for key, value in trial_best.params.items():\n",
    "        print(\"  {}: {}\".format(key, value))\n",
    "else:\n",
    "    train_trial(\"EfficientNet_v2_rw_t\",\n",
    "                \"SGD\",\n",
    "                \"StepLR\",\n",
    "                trial=None,\n",
    "                _forget_rate=0.2, _gradual=50, _num_epoch=200, _num_mixup_alpha=0.1, _iter_per_epoch=200,\n",
    "                _num_exponent=1, _learning_rate=0.04, _freq_evaluate=5, _freeze_weights=False, _train_improve=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# modelA = torch.hub.load('rwightman/pytorch-image-models', 'tf_efficientnetv2_s_in21ft1k')\n",
    "# modelB = torch.hub.load('rwightman/pytorch-image-models', 'tf_efficientnetv2_m_in21ft1k')\n",
    "# modelA = modelA.to(device)\n",
    "# modelB = modelB.to(device)\n",
    "#\n",
    "# _acc1, _acc2 = evaluate(test_loader, modelA, modelB)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.hub.list('rwightman/pytorch-image-models')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# import torch\n",
    "#\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# torch.cuda.empty_cache()\n",
    "# modelC = torch.hub.load('rwightman/pytorch-image-models', 'mobilenetv2_100', pretrained=True)\n",
    "# summary(modelC.to(device), input_size=(3, 32, 32))\n",
    "# modelC\n",
    "\n",
    "\n",
    "# from model import ReshapedPreTrainedModel, modify_pretrained_outputs\n",
    "# modelC = ReshapedPreTrainedModel(modelC, n_outputs=10, dropout_rate=0.25, freeze_weights=True)\n",
    "# modelC.to(device)\n",
    "# modelC = modify_pretrained_outputs(modelC)\n",
    "# modelC\n",
    "# modelC = modify_pretrained_outputs(modelC)\n",
    "# modelC.to(device)\n",
    "#\n",
    "# image, label = next(iter(test_loader))\n",
    "# image = image.to(device)\n",
    "# out = modelC(image)\n",
    "#\n",
    "# print(out.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# study = optuna.load_study(study_name=\"cifar10_1\",\n",
    "#                           storage='mysql://root:1115@192.168.50.11:3306/co_mixing')\n",
    "# print(study.best_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# optuna.delete_study(study_name=\"cifar10_Task1_2022_03_17_compare_nomixup\",\n",
    "#                     storage='mysql://root:1115@192.168.50.11:3306/co_mixing')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from optuna.visualization import plot_param_importances\n",
    "#\n",
    "# plot_param_importances(study=study)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}