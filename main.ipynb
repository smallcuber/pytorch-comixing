{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from model import CNN, ReshapedPreTrainedModel\n",
    "from loss import loss_coteaching\n",
    "from optuna.trial import TrialState\n",
    "import optuna\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mixup import mixup_data\n",
    "from model import dict_models, load_pretrained_model_by_name\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from loader_CIFAR import CifarDataloader, CifarDataset, unpickle\n",
    "from loader_ANIMAL10N import Animal10N\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "# torch.autograd.set_detect_anomaly(True)  this is for debugging\n",
    "\n",
    "name_dataset = 'animal10n'  # either cifar10 or animal10n\n",
    "id_task = 1  # Task 1 to 3, noise file created by TA\n",
    "\n",
    "\n",
    "# path to the data file (don't forget to download the feature data and also put the noisy label file under this folder)\n",
    "if name_dataset == 'animal10n':\n",
    "    data_path = r'rawdata_ANIMAL10N'\n",
    "else:\n",
    "    data_path = r\"C://Users//Wenda//Documents//Pytorch//rawdata_CIFAR10\"\n",
    "\n",
    "num_iter_per_epoch = 100\n",
    "num_print_freq = 100\n",
    "num_epoch = 200\n",
    "\n",
    "if name_dataset == 'animal10n':\n",
    "    num_batch_size = 32  # Have to use a smaller batch for animal10n due to the GPU memory limit (GTX1080 8G)\n",
    "else:\n",
    "    num_batch_size = 128\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    num_batch_size = num_batch_size * torch.cuda.device_count()\n",
    "\n",
    "def tryUsingDataParallel(model):\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using parallel with {torch.cuda.device_count()} GPUs!\")\n",
    "        return torch.nn.DataParallel(model)\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "num_gradual = 10\n",
    "num_exponent = 1\n",
    "num_forget_rate = 0.1\n",
    "num_noise_rate = 0.2\n",
    "num_workers = 6\n",
    "num_classes = 10\n",
    "num_learning_rate = 0.001\n",
    "num_input_channel = 3\n",
    "\n",
    "# Adjust learning rate and betas for Adam Optimizer\n",
    "num_mixup_alpha = 0.1\n",
    "num_epoch_decay_start = 80\n",
    "mom1 = 0.9\n",
    "mom2 = 0.1\n",
    "alpha_plan = [num_learning_rate] * num_epoch\n",
    "beta1_plan = [mom1] * num_epoch\n",
    "for i in range(num_epoch_decay_start, num_epoch):\n",
    "    alpha_plan[i] = float(num_epoch - i) / (num_epoch - num_epoch_decay_start) * num_learning_rate\n",
    "    beta1_plan[i] = mom2\n",
    "\n",
    "rate_schedule = np.ones(num_epoch) * num_forget_rate\n",
    "rate_schedule[:num_gradual] = np.linspace(0, num_forget_rate ** num_exponent, num_gradual)\n",
    "\n",
    "json_noise_file_names = {\n",
    "    1: 'cifar10_noisy_labels_task1.json',\n",
    "    2: 'cifar10_noisy_labels_task2.json',\n",
    "    3: 'cifar10_noisy_labels_task3.json'\n",
    "}\n",
    "noise_file_name = json_noise_file_names[id_task]  # Change The number to switch tasks\n",
    "\n",
    "if name_dataset == 'cifar10':\n",
    "    loader = CifarDataloader(name_dataset, batch_size=128,\n",
    "                             num_workers=10,\n",
    "                             root_dir=data_path,\n",
    "                             noise_file='%s/%s' % (data_path, noise_file_name))\n",
    "    train_loader, noisy_labels, clean_labels = loader.run('train')\n",
    "    noise_or_not = np.transpose(noisy_labels) == np.transpose(clean_labels)\n",
    "    test_loader = loader.run('test')\n",
    "else:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset_train = Animal10N(split='train', data_path=data_path, transform=transform_train)\n",
    "    dataset_test = Animal10N(split='test', data_path=data_path, transform=transform_test)\n",
    "\n",
    "    train_loader = DataLoader(dataset_train, batch_size=num_batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(dataset_test, batch_size=num_batch_size * 2, shuffle=False, num_workers=num_workers)\n",
    "    noise_or_not = None\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if name_dataset == 'animal10n':\n",
    "    model_full_name = f'{name_dataset}'\n",
    "else:\n",
    "    model_full_name = f'{name_dataset}_{noise_file_name}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "#\n",
    "# def imshow(axis, inp):\n",
    "#     \"\"\"Denormalize and show\"\"\"\n",
    "#     inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     mean = np.array([0.485, 0.456, 0.406])\n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     inp = std * inp + mean\n",
    "#     axis.imshow(inp)\n",
    "# img, label, idx = next(iter(train_loader))\n",
    "# print(img.size(), label.size())\n",
    "# fig = plt.figure(1, figsize=(16, 4))\n",
    "# grid = ImageGrid(fig, 111, nrows_ncols=(1, 4), axes_pad=0.05)\n",
    "# for i in range(img.size()[0]):\n",
    "#     ax = grid[i]\n",
    "#     imshow(ax, img[i])\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = alpha_plan[epoch]\n",
    "        param_group['betas'] = (beta1_plan[epoch], 0.999)  # Only change beta1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# TODO: Complete migrating the Co-Teaching model\n",
    "def accuracy(logit, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    output = F.softmax(logit, dim=1)\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def train(loader_train, epoch, model1, optimizer1, model2, optimizer2, criterion, _rate_schedule, _mixup_alpha,\n",
    "          _iter_per_epoch):\n",
    "    print(\"Training... %s\" % model_full_name)\n",
    "    pure_ratio_list = []\n",
    "    _pure_ratio_1_list = []\n",
    "    _pure_ratio_2_list = []\n",
    "    count_total_train1 = 0\n",
    "    count_total_correct1 = 0\n",
    "    count_total_train2 = 0\n",
    "    count_total_correct2 = 0\n",
    "    num_correct_1 = 0\n",
    "    num_correct_2 = 0\n",
    "    num_total = 0\n",
    "    model1.to(device)\n",
    "    model2.to(device)\n",
    "\n",
    "    for i, (images, labels, indexes) in enumerate(loader_train):\n",
    "        # ind = indexes.cpu().numpy().transpose()\n",
    "        ind = indexes.numpy().transpose()\n",
    "        if i > _iter_per_epoch > 0:\n",
    "            break\n",
    "\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        num_total = labels.size(0)\n",
    "\n",
    "        images, label_a, label_b, lam = mixup_data(images, labels, device, _mixup_alpha)\n",
    "\n",
    "        # Forward Backward Optimize\n",
    "        output1 = model1(images)\n",
    "        _, predicted1 = torch.max(output1.data, 1)\n",
    "        prec1, _ = accuracy(output1, labels, topk=(1, 5))\n",
    "        count_total_train1 += 1\n",
    "        count_total_correct1 += prec1\n",
    "\n",
    "        output2 = model2(images)\n",
    "        _, predicted2 = torch.max(output2.data, 1)\n",
    "        prec2, _ = accuracy(output2, labels, topk=(1, 5))\n",
    "        count_total_train2 += 1\n",
    "        count_total_correct2 += prec2\n",
    "\n",
    "        num_correct_1 += (lam * predicted1.eq(label_a.data).cpu().sum().float()\n",
    "                          + (1 - lam) * predicted1.eq(label_b.data).cpu().sum().float())\n",
    "        num_correct_2 += (lam * predicted2.eq(label_a.data).cpu().sum().float()\n",
    "                          + (1 - lam) * predicted2.eq(label_b.data).cpu().sum().float())\n",
    "        # num_correct_1 += (lam * predicted1.eq(label_a.data).cpu().sum().float()\n",
    "        #                   + (1 - lam) * predicted1.eq(label_b.data).cpu().sum().float())\n",
    "        # num_correct_2 += (lam * predicted2.eq(label_a.data).cpu().sum().float()\n",
    "        #                   + (1 - lam) * predicted2.eq(label_b.data).cpu().sum().float())\n",
    "        num_acc_1 = num_correct_1 / num_total\n",
    "        num_acc_2 = num_correct_2 / num_total\n",
    "\n",
    "        loss1, loss2, pure_ratio_1, pure_ratio_2 = loss_coteaching(criterion, output1, output2, labels, label_a,\n",
    "                                                                   label_b, _rate_schedule[epoch], ind, noise_or_not,\n",
    "                                                                   lam)\n",
    "\n",
    "        if pure_ratio_1 and pure_ratio_2 is not None:\n",
    "            _pure_ratio_1_list.append(100 * pure_ratio_1)\n",
    "            _pure_ratio_2_list.append(100 * pure_ratio_2)\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        if (i + 1) % num_print_freq == 0:\n",
    "            if pure_ratio_1 and pure_ratio_2 is not None:\n",
    "                str_calc_pure_ratio = 'Pure Ratio1: %.4f, Pure Ratio2 %.4f' % (\n",
    "                    np.sum(_pure_ratio_1_list) / len(_pure_ratio_1_list),\n",
    "                    np.sum(_pure_ratio_2_list) / len(_pure_ratio_2_list))\n",
    "            else:\n",
    "                str_calc_pure_ratio = 'Animal10N dataset without pure ratio'\n",
    "\n",
    "            print(\n",
    "                'Train Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f, %s'\n",
    "                % (epoch + 1, num_epoch, i + 1, len(train_loader) // num_batch_size, num_acc_1, num_acc_2, loss1.item(),\n",
    "                   loss2.item(), str_calc_pure_ratio))\n",
    "\n",
    "\n",
    "    train_acc1 = float(count_total_correct1) / float(count_total_train1)\n",
    "    train_acc2 = float(count_total_correct2) / float(count_total_train2)\n",
    "    return train_acc1, train_acc2, _pure_ratio_1_list, _pure_ratio_2_list\n",
    "\n",
    "\n",
    "def evaluate(_test_loader, model1, model2, particial_test_rate=0):\n",
    "    print('Evaluating %s...' % model_full_name)\n",
    "    # model1 = model1.to(device)\n",
    "    # model2 = model2.to(device)\n",
    "    model1.eval()  # Change model to 'eval' mode.\n",
    "    correct1 = 0\n",
    "    total1 = 0\n",
    "    print(\"Start evaluating model 1\")\n",
    "    for images, labels in _test_loader:\n",
    "        images = Variable(images).to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits1 = model1(images)\n",
    "        outputs1 = F.softmax(logits1, dim=1)\n",
    "        _, pred1 = torch.max(outputs1.data, 1)\n",
    "        total1 += labels.size(0)\n",
    "        correct1 += (pred1 == labels).sum()\n",
    "\n",
    "    print(\"Start evaluating model 2\")\n",
    "    model2.eval()  # Change model to 'eval' mode\n",
    "    correct2 = 0\n",
    "    total2 = 0\n",
    "    for images, labels in _test_loader:\n",
    "        images = Variable(images).to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits2 = model2(images)\n",
    "        outputs2 = F.softmax(logits2, dim=1)\n",
    "        _, pred2 = torch.max(outputs2.data, 1)\n",
    "        total2 += labels.size(0)\n",
    "        correct2 += (pred2 == labels).sum()\n",
    "\n",
    "    acc1 = 100 * float(correct1) / float(total1)\n",
    "    acc2 = 100 * float(correct2) / float(total2)\n",
    "    return acc1, acc2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "def main_single_train():\n",
    "    # Train the model\n",
    "\n",
    "    # Data Loader (Input Pipeline)\n",
    "    print('loading dataset...')\n",
    "\n",
    "    # Define models\n",
    "    print('building model...')\n",
    "    cnn1 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "    # cnn1 = ReshapedPreTrainedModel(modelC, n_outputs=10, dropout_rate=0.25, freeze_weights=True)\n",
    "    cnn1 = tryUsingDataParallel(cnn1)\n",
    "    cnn1.to(device)\n",
    "    # print(cnn1.parameters)\n",
    "    optimizer1 = torch.optim.Adam(cnn1.parameters(), lr=num_learning_rate)\n",
    "\n",
    "    cnn2 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "    # cnn2 = ReshapedPreTrainedModel(modelC, n_outputs=10, dropout_rate=0.25, freeze_weights=True)\n",
    "    cnn2 = tryUsingDataParallel(cnn2)\n",
    "    cnn2.to(device)\n",
    "    # print(cnn2.parameters)\n",
    "    optimizer2 = torch.optim.Adam(cnn2.parameters(), lr=num_learning_rate)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduce=False)\n",
    "\n",
    "    # training\n",
    "    for epoch in range(1, num_epoch):\n",
    "        # train models\n",
    "        cnn1.train()\n",
    "        adjust_learning_rate(optimizer1, epoch)\n",
    "        cnn2.train()\n",
    "        adjust_learning_rate(optimizer2, epoch)\n",
    "        train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list = train(train_loader, epoch, cnn1, optimizer1,\n",
    "                                                                             cnn2, optimizer2, criterion, rate_schedule,\n",
    "                                                                             num_mixup_alpha, num_iter_per_epoch)\n",
    "        # evaluate models\n",
    "        test_acc1, test_acc2 = evaluate(test_loader, cnn1, cnn2)\n",
    "        # save results\n",
    "        if name_dataset == 'cifar10n':\n",
    "            mean_pure_ratio1 = sum(pure_ratio_1_list) / len(pure_ratio_1_list)\n",
    "            mean_pure_ratio2 = sum(pure_ratio_2_list) / len(pure_ratio_2_list)\n",
    "            str_ratio = 'Pure Ratio 1 %.4f %%, Pure Ratio 2 %.4f' % (mean_pure_ratio1, mean_pure_ratio2)\n",
    "        else:\n",
    "            str_ratio = 'animal10n without pure ratio.'\n",
    "        print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %%, %s' % (\n",
    "            epoch + 1, num_epoch, len(test_loader.dataset), test_acc1, test_acc2, str_ratio))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train_trial(_model_select,\n",
    "                _optimizer_select,\n",
    "                _learning_rate_scheduler_select,\n",
    "                trial,\n",
    "                _forget_rate=0.1, _gradual=10, _num_epoch=200, _num_mixup_alpha=0.1, _iter_per_epoch=200,\n",
    "                _num_exponent=1, _learning_rate=0.02, _freq_evaluate=1, _freeze_weights=True):\n",
    "    _rate_schedule = np.ones(_num_epoch) * _forget_rate\n",
    "    _rate_schedule[:_gradual] = np.linspace(0, _forget_rate ** _num_exponent, _gradual)\n",
    "\n",
    "    print('Start trail...')\n",
    "    # Define models\n",
    "    print('building model...')\n",
    "    if _model_select != \"CNN\":\n",
    "        print(f'Loading pre-trained model: {_model_select}')\n",
    "        _model1 = load_pretrained_model_by_name(_model_select, is_pretrained=True)\n",
    "        _model2 = load_pretrained_model_by_name(_model_select, is_pretrained=True)\n",
    "\n",
    "        if _freeze_weights:\n",
    "            for param in _model1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in _model2.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_input = _model1.classifier.in_features\n",
    "        _model1.classifier = torch.nn.Linear(num_input, num_classes, bias=True).to(device)\n",
    "        num_input = _model2.classifier.in_features\n",
    "        _model2.classifier = torch.nn.Linear(num_input, num_classes, bias=True).to(device)\n",
    "        # _model1 = modify_pretrained_outputs(_model1.to(device), num_output=num_classes, freeze_parameters=_freeze_weights)\n",
    "        # _model1 = modify_pretrained_outputs(_model2.to(device), num_output=num_classes, freeze_parameters=_freeze_weights)\n",
    "    else:\n",
    "        _model1 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "        _model2 = CNN(input_channel=num_input_channel, n_outputs=num_classes)\n",
    "\n",
    "    _model1 = tryUsingDataParallel(_model1)\n",
    "    _model2 = tryUsingDataParallel(_model2)\n",
    "    _model1.to(device)\n",
    "    _model2.to(device)\n",
    "\n",
    "    if _optimizer_select == \"Adam\":\n",
    "        _optm1 = torch.optim.Adam(_model1.parameters(), lr=_learning_rate)\n",
    "        _optm2 = torch.optim.Adam(_model2.parameters(), lr=_learning_rate)\n",
    "    else:\n",
    "        _optm1 = torch.optim.SGD(_model1.parameters(), lr=_learning_rate)\n",
    "        _optm2 = torch.optim.SGD(_model2.parameters(), lr=_learning_rate)\n",
    "\n",
    "    if _learning_rate_scheduler_select == \"StepLR\":\n",
    "        scheduler1 = lr_scheduler.StepLR(_optm1, step_size=30, gamma=0.5)\n",
    "        scheduler2 = lr_scheduler.StepLR(_optm2, step_size=30, gamma=0.5)\n",
    "\n",
    "    _criterion_select = torch.nn.CrossEntropyLoss(reduce=False)\n",
    "    _mean_pure_ratio1 = 0\n",
    "    _mean_pure_ratio2 = 0\n",
    "    _epoch = 0\n",
    "    _train_acc1 = 0\n",
    "    _train_acc2 = 0\n",
    "    _test_acc1, _test_acc2 = 0, 0\n",
    "\n",
    "    print(f\"Current model: {_model_select}, current optimizer: {_optimizer_select}, criterion: {_criterion_select}, lr schedule: {_learning_rate_scheduler_select}, forget rate: {_forget_rate}, gradual rate: {_gradual}, mixup alpha: {_num_mixup_alpha}\")\n",
    "\n",
    "    for _epoch in range(_num_epoch):\n",
    "        # train models\n",
    "        _model1.train()\n",
    "        _model2.train()\n",
    "\n",
    "        _train_acc1, _train_acc2, _pure_ratio_1_list, _pure_ratio_2_list = train(train_loader, _epoch, _model1, _optm1,\n",
    "                                                                                 _model2, _optm2, _criterion_select,\n",
    "                                                                                 _rate_schedule, _num_mixup_alpha,\n",
    "                                                                                 _iter_per_epoch)\n",
    "\n",
    "        if _learning_rate_scheduler_select == \"StepLR\":\n",
    "            scheduler1.step(_epoch)\n",
    "            scheduler2.step(_epoch)\n",
    "        else:\n",
    "            adjust_learning_rate(_optm1, _epoch)\n",
    "            adjust_learning_rate(_optm2, _epoch)\n",
    "\n",
    "        if (_epoch + 1) % _freq_evaluate == 0:\n",
    "            # evaluate models\n",
    "            _test_acc1, _test_acc2 = evaluate(test_loader, _model1, _model2)\n",
    "            # save results\n",
    "            if name_dataset == 'cifar10':\n",
    "                _mean_pure_ratio1 = sum(_pure_ratio_1_list) / len(_pure_ratio_1_list)\n",
    "                _mean_pure_ratio2 = sum(_pure_ratio_2_list) / len(_pure_ratio_2_list)\n",
    "                _str_ratio = 'Pure Ratio 1 %.4f %%, Pure Ratio 2 %.4f' % (_mean_pure_ratio1, _mean_pure_ratio2)\n",
    "            else:\n",
    "                _str_ratio = 'animal10n without pure ratio.'\n",
    "            print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %%, %s' % (\n",
    "                _epoch + 1, num_epoch, len(test_loader.dataset), _test_acc1, _test_acc2, _str_ratio))\n",
    "\n",
    "            _temp_acc = max(_test_acc1, _test_acc2)  # current best testing accuracy\n",
    "            if trial is not None:\n",
    "                trial.report(_temp_acc, _epoch)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "                else:\n",
    "                    print(\"Trial not pruned yet\")\n",
    "            else:\n",
    "                print(\"Trial input is none, assume the study is a single test run\")\n",
    "\n",
    "    return max(_test_acc1, _test_acc2)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    #  _criterion_select = trial.suggest_categorical(\n",
    "    #      \"Criterion\",\n",
    "    #      [\"CEL\"])\n",
    "    _forget_rate_selection = trial.suggest_categorical(\n",
    "        \"Forget Rate\",\n",
    "        [0.05, 0.1, 0.2, 0.5])\n",
    "    _gradual = trial.suggest_categorical(\n",
    "        \"Gradual\",\n",
    "        [10, 50])\n",
    "    _mixup_alpha = trial.suggest_categorical(\n",
    "        \"Mixup Alpha\",\n",
    "        [0, 0.05, 0.1, 0.5])\n",
    "    _optm_select = trial.suggest_categorical(\n",
    "        \"Optimizer\",\n",
    "        [\"SGD\", \"Adam\"])\n",
    "    _lr_scheduler_select = trial.suggest_categorical(\n",
    "        \"Scheduler\",\n",
    "        [\"StepLR\", \"None\"]\n",
    "    )\n",
    "    _lr_select = trial.suggest_categorical(\n",
    "        \"Learning Rate\",\n",
    "        [0.08, 0.04, 0.02, 0.008, 0.004]\n",
    "    )\n",
    "    _model_select = trial.suggest_categorical(  # TODO: Search models for training\n",
    "        \"Model\",\n",
    "        [\"CNN\",\n",
    "         \"Densenet121\",\n",
    "         \"EfficientNet_v2_rw_t\", \"EfficientNet_v2_s\", \"EfficientNet_v2_m\", \"Mobilenet_v2_035\", \"Mobilenet_v2_100\"]\n",
    "    )\n",
    "\n",
    "    _accuracy = train_trial(_model_select,\n",
    "                            _optm_select,\n",
    "                            _lr_scheduler_select,\n",
    "                            trial=trial,\n",
    "                            _forget_rate=_forget_rate_selection, _gradual=_gradual, _num_epoch=200,\n",
    "                            _num_mixup_alpha=_mixup_alpha, _iter_per_epoch=200, _num_exponent=1, _learning_rate=0.04,\n",
    "                            _freq_evaluate=5, _freeze_weights=False)\n",
    "    return _accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run the study here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-04-03 20:57:20,732]\u001B[0m Using an existing study with name 'animal10n_unfreeze_weight_latest_additional_models' instead of creating a new one.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trail...\n",
      "building model...\n",
      "Loading pre-trained model: Mobilenet_v2_100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Wenda/.cache\\torch\\hub\\rwightman_pytorch-image-models_master\n",
      "Using cache found in C:\\Users\\Wenda/.cache\\torch\\hub\\rwightman_pytorch-image-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parallel with 2 GPUs!\n",
      "Using parallel with 2 GPUs!\n",
      "Current model: Mobilenet_v2_100, current optimizer: SGD, criterion: CrossEntropyLoss(), lr schedule: StepLR, forget rate: 0.05, gradual rate: 10, mixup alpha: 0.1\n",
      "Training... animal10n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [1/200], Iter [100/12] Training Accuracy1: 34.1769, Training Accuracy2: 34.2787, Loss1: 1.5805, Loss2: 1.5849, Animal10N dataset without pure ratio\n",
      "Train Epoch [1/200], Iter [200/12] Training Accuracy1: 80.8579, Training Accuracy2: 81.7679, Loss1: 1.3124, Loss2: 1.4678, Animal10N dataset without pure ratio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... animal10n\n",
      "Train Epoch [2/200], Iter [100/12] Training Accuracy1: 51.4653, Training Accuracy2: 51.5232, Loss1: 1.8268, Loss2: 1.8374, Animal10N dataset without pure ratio\n",
      "Train Epoch [2/200], Iter [200/12] Training Accuracy1: 105.7418, Training Accuracy2: 105.7695, Loss1: 2.1974, Loss2: 2.3388, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [3/200], Iter [100/12] Training Accuracy1: 54.8962, Training Accuracy2: 54.8427, Loss1: 1.1363, Loss2: 1.0844, Animal10N dataset without pure ratio\n",
      "Train Epoch [3/200], Iter [200/12] Training Accuracy1: 110.0221, Training Accuracy2: 109.8787, Loss1: 1.0716, Loss2: 1.0560, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [4/200], Iter [100/12] Training Accuracy1: 57.7918, Training Accuracy2: 57.6011, Loss1: 1.0161, Loss2: 1.1434, Animal10N dataset without pure ratio\n",
      "Train Epoch [4/200], Iter [200/12] Training Accuracy1: 114.9296, Training Accuracy2: 114.8833, Loss1: 1.1891, Loss2: 1.1476, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [5/200], Iter [100/12] Training Accuracy1: 58.9241, Training Accuracy2: 58.6340, Loss1: 1.0302, Loss2: 0.9605, Animal10N dataset without pure ratio\n",
      "Train Epoch [5/200], Iter [200/12] Training Accuracy1: 118.7995, Training Accuracy2: 118.3678, Loss1: 0.9650, Loss2: 0.9438, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-04-03 21:00:46,852]\u001B[0m Trial 179 pruned. \u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/200] Test Accuracy on the 5000 test images: Model1 69.4400 % Model2 68.4400 %, animal10n without pure ratio.\n",
      "Start trail...\n",
      "building model...\n",
      "Loading pre-trained model: EfficientNet_v2_m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Wenda/.cache\\torch\\hub\\rwightman_pytorch-image-models_master\n",
      "Using cache found in C:\\Users\\Wenda/.cache\\torch\\hub\\rwightman_pytorch-image-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parallel with 2 GPUs!\n",
      "Using parallel with 2 GPUs!\n",
      "Current model: EfficientNet_v2_m, current optimizer: SGD, criterion: CrossEntropyLoss(), lr schedule: StepLR, forget rate: 0.05, gradual rate: 10, mixup alpha: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... animal10n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [1/200], Iter [100/12] Training Accuracy1: 39.4620, Training Accuracy2: 38.9672, Loss1: 1.4522, Loss2: 1.4104, Animal10N dataset without pure ratio\n",
      "Train Epoch [1/200], Iter [200/12] Training Accuracy1: 91.5465, Training Accuracy2: 91.5111, Loss1: 1.0944, Loss2: 1.1471, Animal10N dataset without pure ratio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... animal10n\n",
      "Train Epoch [2/200], Iter [100/12] Training Accuracy1: 57.9013, Training Accuracy2: 57.1583, Loss1: 1.1860, Loss2: 1.1820, Animal10N dataset without pure ratio\n",
      "Train Epoch [2/200], Iter [200/12] Training Accuracy1: 118.3071, Training Accuracy2: 117.5280, Loss1: 0.8919, Loss2: 0.9688, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [3/200], Iter [100/12] Training Accuracy1: 63.4985, Training Accuracy2: 64.2717, Loss1: 0.8538, Loss2: 0.9012, Animal10N dataset without pure ratio\n",
      "Train Epoch [3/200], Iter [200/12] Training Accuracy1: 127.3854, Training Accuracy2: 128.3325, Loss1: 1.0420, Loss2: 1.0958, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [4/200], Iter [100/12] Training Accuracy1: 66.7322, Training Accuracy2: 66.8795, Loss1: 0.7478, Loss2: 0.8298, Animal10N dataset without pure ratio\n",
      "Train Epoch [4/200], Iter [200/12] Training Accuracy1: 135.4537, Training Accuracy2: 135.5587, Loss1: 0.7696, Loss2: 0.7421, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [5/200], Iter [100/12] Training Accuracy1: 68.6646, Training Accuracy2: 67.9628, Loss1: 0.7100, Loss2: 0.7510, Animal10N dataset without pure ratio\n",
      "Train Epoch [5/200], Iter [200/12] Training Accuracy1: 139.3344, Training Accuracy2: 137.8769, Loss1: 0.6455, Loss2: 0.5586, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [5/200] Test Accuracy on the 5000 test images: Model1 73.9400 % Model2 74.3400 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [6/200], Iter [100/12] Training Accuracy1: 72.2868, Training Accuracy2: 71.5784, Loss1: 0.8921, Loss2: 0.9335, Animal10N dataset without pure ratio\n",
      "Train Epoch [6/200], Iter [200/12] Training Accuracy1: 141.6319, Training Accuracy2: 140.8993, Loss1: 0.6914, Loss2: 0.6566, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [7/200], Iter [100/12] Training Accuracy1: 72.3004, Training Accuracy2: 71.8742, Loss1: 0.5032, Loss2: 0.5070, Animal10N dataset without pure ratio\n",
      "Train Epoch [7/200], Iter [200/12] Training Accuracy1: 146.4833, Training Accuracy2: 146.1209, Loss1: 0.6554, Loss2: 0.5678, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [8/200], Iter [100/12] Training Accuracy1: 73.5261, Training Accuracy2: 74.0104, Loss1: 0.6223, Loss2: 0.6620, Animal10N dataset without pure ratio\n",
      "Train Epoch [8/200], Iter [200/12] Training Accuracy1: 146.7986, Training Accuracy2: 147.1465, Loss1: 0.6531, Loss2: 0.5801, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [9/200], Iter [100/12] Training Accuracy1: 74.8202, Training Accuracy2: 73.9652, Loss1: 0.9072, Loss2: 0.8439, Animal10N dataset without pure ratio\n",
      "Train Epoch [9/200], Iter [200/12] Training Accuracy1: 148.9043, Training Accuracy2: 147.6028, Loss1: 0.7426, Loss2: 0.6587, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [10/200], Iter [100/12] Training Accuracy1: 75.6960, Training Accuracy2: 75.2898, Loss1: 0.4944, Loss2: 0.5189, Animal10N dataset without pure ratio\n",
      "Train Epoch [10/200], Iter [200/12] Training Accuracy1: 152.8809, Training Accuracy2: 151.8437, Loss1: 0.6084, Loss2: 0.5200, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [10/200] Test Accuracy on the 5000 test images: Model1 78.2200 % Model2 77.7800 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [11/200], Iter [100/12] Training Accuracy1: 77.7160, Training Accuracy2: 78.4020, Loss1: 0.4594, Loss2: 0.4694, Animal10N dataset without pure ratio\n",
      "Train Epoch [11/200], Iter [200/12] Training Accuracy1: 153.5888, Training Accuracy2: 154.9871, Loss1: 0.5057, Loss2: 0.4153, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [12/200], Iter [100/12] Training Accuracy1: 77.9077, Training Accuracy2: 78.2886, Loss1: 0.4223, Loss2: 0.3083, Animal10N dataset without pure ratio\n",
      "Train Epoch [12/200], Iter [200/12] Training Accuracy1: 157.6701, Training Accuracy2: 158.0081, Loss1: 0.2450, Loss2: 0.3294, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [13/200], Iter [100/12] Training Accuracy1: 79.2179, Training Accuracy2: 79.3492, Loss1: 0.3580, Loss2: 0.4040, Animal10N dataset without pure ratio\n",
      "Train Epoch [13/200], Iter [200/12] Training Accuracy1: 157.7616, Training Accuracy2: 157.9285, Loss1: 0.4431, Loss2: 0.5584, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [14/200], Iter [100/12] Training Accuracy1: 80.7799, Training Accuracy2: 80.9957, Loss1: 0.3280, Loss2: 0.3762, Animal10N dataset without pure ratio\n",
      "Train Epoch [14/200], Iter [200/12] Training Accuracy1: 162.1445, Training Accuracy2: 162.3947, Loss1: 0.1276, Loss2: 0.2126, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [15/200], Iter [100/12] Training Accuracy1: 79.4439, Training Accuracy2: 79.3620, Loss1: 0.2962, Loss2: 0.3498, Animal10N dataset without pure ratio\n",
      "Train Epoch [15/200], Iter [200/12] Training Accuracy1: 160.1193, Training Accuracy2: 159.6210, Loss1: 2.0141, Loss2: 1.9502, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [15/200] Test Accuracy on the 5000 test images: Model1 79.1000 % Model2 79.3000 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [16/200], Iter [100/12] Training Accuracy1: 80.5267, Training Accuracy2: 81.0810, Loss1: 0.5694, Loss2: 0.4282, Animal10N dataset without pure ratio\n",
      "Train Epoch [16/200], Iter [200/12] Training Accuracy1: 163.5287, Training Accuracy2: 163.3987, Loss1: 0.2102, Loss2: 0.1352, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [17/200], Iter [100/12] Training Accuracy1: 81.8169, Training Accuracy2: 81.6692, Loss1: 0.2466, Loss2: 0.2299, Animal10N dataset without pure ratio\n",
      "Train Epoch [17/200], Iter [200/12] Training Accuracy1: 165.1286, Training Accuracy2: 164.4796, Loss1: 0.3380, Loss2: 0.3391, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [18/200], Iter [100/12] Training Accuracy1: 83.3384, Training Accuracy2: 82.8126, Loss1: 0.2435, Loss2: 0.2851, Animal10N dataset without pure ratio\n",
      "Train Epoch [18/200], Iter [200/12] Training Accuracy1: 166.4064, Training Accuracy2: 166.0147, Loss1: 0.2865, Loss2: 0.3122, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [19/200], Iter [100/12] Training Accuracy1: 81.7025, Training Accuracy2: 81.5766, Loss1: 1.6134, Loss2: 1.6608, Animal10N dataset without pure ratio\n",
      "Train Epoch [19/200], Iter [200/12] Training Accuracy1: 164.6089, Training Accuracy2: 164.3492, Loss1: 0.3011, Loss2: 0.3351, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [20/200], Iter [100/12] Training Accuracy1: 81.7568, Training Accuracy2: 82.2913, Loss1: 0.4569, Loss2: 0.3703, Animal10N dataset without pure ratio\n",
      "Train Epoch [20/200], Iter [200/12] Training Accuracy1: 166.5885, Training Accuracy2: 166.9533, Loss1: 0.1534, Loss2: 0.1944, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [20/200] Test Accuracy on the 5000 test images: Model1 80.3000 % Model2 79.5000 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [21/200], Iter [100/12] Training Accuracy1: 83.6128, Training Accuracy2: 83.6123, Loss1: 0.2989, Loss2: 0.3398, Animal10N dataset without pure ratio\n",
      "Train Epoch [21/200], Iter [200/12] Training Accuracy1: 168.3415, Training Accuracy2: 168.2840, Loss1: 0.3172, Loss2: 0.2788, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [22/200], Iter [100/12] Training Accuracy1: 84.5345, Training Accuracy2: 84.7528, Loss1: 0.1995, Loss2: 0.2428, Animal10N dataset without pure ratio\n",
      "Train Epoch [22/200], Iter [200/12] Training Accuracy1: 166.4100, Training Accuracy2: 166.9618, Loss1: 1.6903, Loss2: 1.7892, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [23/200], Iter [100/12] Training Accuracy1: 82.8702, Training Accuracy2: 83.3546, Loss1: 0.3434, Loss2: 0.4097, Animal10N dataset without pure ratio\n",
      "Train Epoch [23/200], Iter [200/12] Training Accuracy1: 167.7936, Training Accuracy2: 167.5524, Loss1: 0.1397, Loss2: 0.1880, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [24/200], Iter [100/12] Training Accuracy1: 85.8233, Training Accuracy2: 85.9009, Loss1: 0.1398, Loss2: 0.0847, Animal10N dataset without pure ratio\n",
      "Train Epoch [24/200], Iter [200/12] Training Accuracy1: 171.1919, Training Accuracy2: 171.9805, Loss1: 0.1481, Loss2: 0.2062, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [25/200], Iter [100/12] Training Accuracy1: 84.7043, Training Accuracy2: 85.2216, Loss1: 0.1023, Loss2: 0.1688, Animal10N dataset without pure ratio\n",
      "Train Epoch [25/200], Iter [200/12] Training Accuracy1: 168.3298, Training Accuracy2: 169.0364, Loss1: 0.1409, Loss2: 0.1753, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-04-03 22:01:21,354]\u001B[0m Trial 180 pruned. \u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/200] Test Accuracy on the 5000 test images: Model1 80.1600 % Model2 79.1400 %, animal10n without pure ratio.\n",
      "Start trail...\n",
      "building model...\n",
      "Loading pre-trained model: EfficientNet_v2_m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Wenda/.cache\\torch\\hub\\rwightman_pytorch-image-models_master\n",
      "Using cache found in C:\\Users\\Wenda/.cache\\torch\\hub\\rwightman_pytorch-image-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parallel with 2 GPUs!\n",
      "Using parallel with 2 GPUs!\n",
      "Current model: EfficientNet_v2_m, current optimizer: SGD, criterion: CrossEntropyLoss(), lr schedule: StepLR, forget rate: 0.2, gradual rate: 50, mixup alpha: 0.05\n",
      "Training... animal10n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [1/200], Iter [100/12] Training Accuracy1: 41.0809, Training Accuracy2: 39.7904, Loss1: 1.5865, Loss2: 1.6058, Animal10N dataset without pure ratio\n",
      "Train Epoch [1/200], Iter [200/12] Training Accuracy1: 92.9760, Training Accuracy2: 92.5008, Loss1: 1.2655, Loss2: 1.1097, Animal10N dataset without pure ratio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... animal10n\n",
      "Train Epoch [2/200], Iter [100/12] Training Accuracy1: 58.1238, Training Accuracy2: 58.3829, Loss1: 1.1651, Loss2: 1.1454, Animal10N dataset without pure ratio\n",
      "Train Epoch [2/200], Iter [200/12] Training Accuracy1: 119.4679, Training Accuracy2: 119.7916, Loss1: 1.0515, Loss2: 1.0071, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [3/200], Iter [100/12] Training Accuracy1: 67.2011, Training Accuracy2: 66.8627, Loss1: 0.7676, Loss2: 0.8660, Animal10N dataset without pure ratio\n",
      "Train Epoch [3/200], Iter [200/12] Training Accuracy1: 132.2717, Training Accuracy2: 132.1163, Loss1: 1.0287, Loss2: 0.9675, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [4/200], Iter [100/12] Training Accuracy1: 68.0198, Training Accuracy2: 67.2238, Loss1: 0.6937, Loss2: 0.7252, Animal10N dataset without pure ratio\n",
      "Train Epoch [4/200], Iter [200/12] Training Accuracy1: 134.6657, Training Accuracy2: 133.5798, Loss1: 1.0092, Loss2: 1.0855, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [5/200], Iter [100/12] Training Accuracy1: 67.3146, Training Accuracy2: 67.5185, Loss1: 0.9199, Loss2: 0.8505, Animal10N dataset without pure ratio\n",
      "Train Epoch [5/200], Iter [200/12] Training Accuracy1: 138.2261, Training Accuracy2: 138.2459, Loss1: 0.8254, Loss2: 0.7372, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [5/200] Test Accuracy on the 5000 test images: Model1 75.2000 % Model2 75.1800 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [6/200], Iter [100/12] Training Accuracy1: 71.2202, Training Accuracy2: 71.5653, Loss1: 0.7664, Loss2: 0.6917, Animal10N dataset without pure ratio\n",
      "Train Epoch [6/200], Iter [200/12] Training Accuracy1: 142.7492, Training Accuracy2: 143.1544, Loss1: 0.6584, Loss2: 0.5938, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [7/200], Iter [100/12] Training Accuracy1: 73.2163, Training Accuracy2: 73.3879, Loss1: 1.3072, Loss2: 1.3944, Animal10N dataset without pure ratio\n",
      "Train Epoch [7/200], Iter [200/12] Training Accuracy1: 145.8582, Training Accuracy2: 145.8020, Loss1: 0.7756, Loss2: 0.7804, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [8/200], Iter [100/12] Training Accuracy1: 73.8461, Training Accuracy2: 74.3937, Loss1: 0.5500, Loss2: 0.6017, Animal10N dataset without pure ratio\n",
      "Train Epoch [8/200], Iter [200/12] Training Accuracy1: 146.9731, Training Accuracy2: 147.3783, Loss1: 0.6651, Loss2: 0.6850, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [9/200], Iter [100/12] Training Accuracy1: 75.4149, Training Accuracy2: 74.9726, Loss1: 0.6699, Loss2: 0.6327, Animal10N dataset without pure ratio\n",
      "Train Epoch [9/200], Iter [200/12] Training Accuracy1: 150.6483, Training Accuracy2: 150.2730, Loss1: 0.4615, Loss2: 0.5472, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [10/200], Iter [100/12] Training Accuracy1: 75.7923, Training Accuracy2: 75.5544, Loss1: 0.3579, Loss2: 0.3134, Animal10N dataset without pure ratio\n",
      "Train Epoch [10/200], Iter [200/12] Training Accuracy1: 152.6120, Training Accuracy2: 152.6643, Loss1: 0.5520, Loss2: 0.5515, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [10/200] Test Accuracy on the 5000 test images: Model1 76.9200 % Model2 77.9400 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [11/200], Iter [100/12] Training Accuracy1: 77.8387, Training Accuracy2: 78.7969, Loss1: 0.4660, Loss2: 0.4807, Animal10N dataset without pure ratio\n",
      "Train Epoch [11/200], Iter [200/12] Training Accuracy1: 154.6244, Training Accuracy2: 155.5603, Loss1: 0.5116, Loss2: 0.5585, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [12/200], Iter [100/12] Training Accuracy1: 78.5204, Training Accuracy2: 78.5631, Loss1: 0.4533, Loss2: 0.4453, Animal10N dataset without pure ratio\n",
      "Train Epoch [12/200], Iter [200/12] Training Accuracy1: 155.9003, Training Accuracy2: 155.7715, Loss1: 0.6830, Loss2: 0.6122, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [13/200], Iter [100/12] Training Accuracy1: 80.9156, Training Accuracy2: 80.7744, Loss1: 0.4172, Loss2: 0.3979, Animal10N dataset without pure ratio\n",
      "Train Epoch [13/200], Iter [200/12] Training Accuracy1: 161.0768, Training Accuracy2: 161.2187, Loss1: 0.3739, Loss2: 0.4244, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [14/200], Iter [100/12] Training Accuracy1: 79.8808, Training Accuracy2: 80.1985, Loss1: 0.5051, Loss2: 0.5299, Animal10N dataset without pure ratio\n",
      "Train Epoch [14/200], Iter [200/12] Training Accuracy1: 159.9769, Training Accuracy2: 160.3044, Loss1: 0.5591, Loss2: 0.5483, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [15/200], Iter [100/12] Training Accuracy1: 79.6740, Training Accuracy2: 79.9173, Loss1: 0.3257, Loss2: 0.3602, Animal10N dataset without pure ratio\n",
      "Train Epoch [15/200], Iter [200/12] Training Accuracy1: 160.3034, Training Accuracy2: 160.7267, Loss1: 0.3840, Loss2: 0.4086, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [15/200] Test Accuracy on the 5000 test images: Model1 79.3200 % Model2 78.9600 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [16/200], Iter [100/12] Training Accuracy1: 82.0528, Training Accuracy2: 81.8182, Loss1: 0.2842, Loss2: 0.2949, Animal10N dataset without pure ratio\n",
      "Train Epoch [16/200], Iter [200/12] Training Accuracy1: 163.7070, Training Accuracy2: 163.0802, Loss1: 0.2159, Loss2: 0.2158, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [17/200], Iter [100/12] Training Accuracy1: 81.3565, Training Accuracy2: 81.5803, Loss1: 0.4556, Loss2: 0.4387, Animal10N dataset without pure ratio\n",
      "Train Epoch [17/200], Iter [200/12] Training Accuracy1: 162.5965, Training Accuracy2: 163.1810, Loss1: 0.2778, Loss2: 0.3216, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [18/200], Iter [100/12] Training Accuracy1: 81.3952, Training Accuracy2: 81.7785, Loss1: 0.3188, Loss2: 0.2177, Animal10N dataset without pure ratio\n",
      "Train Epoch [18/200], Iter [200/12] Training Accuracy1: 163.6046, Training Accuracy2: 164.0336, Loss1: 0.3614, Loss2: 0.4032, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [19/200], Iter [100/12] Training Accuracy1: 82.7587, Training Accuracy2: 82.8119, Loss1: 0.1866, Loss2: 0.1939, Animal10N dataset without pure ratio\n",
      "Train Epoch [19/200], Iter [200/12] Training Accuracy1: 164.7324, Training Accuracy2: 164.5135, Loss1: 0.4283, Loss2: 0.4250, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [20/200], Iter [100/12] Training Accuracy1: 83.9540, Training Accuracy2: 83.9742, Loss1: 0.2932, Loss2: 0.3399, Animal10N dataset without pure ratio\n",
      "Train Epoch [20/200], Iter [200/12] Training Accuracy1: 166.8596, Training Accuracy2: 166.9326, Loss1: 0.2076, Loss2: 0.2171, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [20/200] Test Accuracy on the 5000 test images: Model1 79.5000 % Model2 80.5400 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [21/200], Iter [100/12] Training Accuracy1: 85.5331, Training Accuracy2: 85.9541, Loss1: 0.2406, Loss2: 0.1204, Animal10N dataset without pure ratio\n",
      "Train Epoch [21/200], Iter [200/12] Training Accuracy1: 170.2949, Training Accuracy2: 170.4514, Loss1: 0.0909, Loss2: 0.1093, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [22/200], Iter [100/12] Training Accuracy1: 84.0527, Training Accuracy2: 84.3784, Loss1: 0.2349, Loss2: 0.1780, Animal10N dataset without pure ratio\n",
      "Train Epoch [22/200], Iter [200/12] Training Accuracy1: 166.9997, Training Accuracy2: 166.8594, Loss1: 0.2307, Loss2: 0.1676, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [23/200], Iter [100/12] Training Accuracy1: 83.8416, Training Accuracy2: 83.9357, Loss1: 0.1598, Loss2: 0.2574, Animal10N dataset without pure ratio\n",
      "Train Epoch [23/200], Iter [200/12] Training Accuracy1: 167.5245, Training Accuracy2: 168.2401, Loss1: 1.0067, Loss2: 1.0582, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [24/200], Iter [100/12] Training Accuracy1: 85.4318, Training Accuracy2: 85.2244, Loss1: 1.6432, Loss2: 1.5551, Animal10N dataset without pure ratio\n",
      "Train Epoch [24/200], Iter [200/12] Training Accuracy1: 169.8369, Training Accuracy2: 169.6682, Loss1: 0.2436, Loss2: 0.3808, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [25/200], Iter [100/12] Training Accuracy1: 84.5932, Training Accuracy2: 84.3871, Loss1: 0.3854, Loss2: 0.2737, Animal10N dataset without pure ratio\n",
      "Train Epoch [25/200], Iter [200/12] Training Accuracy1: 168.7718, Training Accuracy2: 168.9642, Loss1: 0.2499, Loss2: 0.1411, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-04-03 23:02:30,022]\u001B[0m Trial 181 pruned. \u001B[0m\n",
      "Using cache found in C:\\Users\\Wenda/.cache\\torch\\hub\\rwightman_pytorch-image-models_master\n",
      "No pretrained weights exist for this model. Using random initialization.\n",
      "Using cache found in C:\\Users\\Wenda/.cache\\torch\\hub\\rwightman_pytorch-image-models_master\n",
      "No pretrained weights exist for this model. Using random initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/200] Test Accuracy on the 5000 test images: Model1 80.1400 % Model2 80.3800 %, animal10n without pure ratio.\n",
      "Start trail...\n",
      "building model...\n",
      "Loading pre-trained model: Mobilenet_v2_035\n",
      "Using parallel with 2 GPUs!\n",
      "Using parallel with 2 GPUs!\n",
      "Current model: Mobilenet_v2_035, current optimizer: SGD, criterion: CrossEntropyLoss(), lr schedule: StepLR, forget rate: 0.2, gradual rate: 10, mixup alpha: 0.05\n",
      "Training... animal10n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [1/200], Iter [100/12] Training Accuracy1: 11.5060, Training Accuracy2: 11.0043, Loss1: 2.3080, Loss2: 2.3614, Animal10N dataset without pure ratio\n",
      "Train Epoch [1/200], Iter [200/12] Training Accuracy1: 24.7449, Training Accuracy2: 22.9613, Loss1: 2.3864, Loss2: 2.3665, Animal10N dataset without pure ratio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... animal10n\n",
      "Train Epoch [2/200], Iter [100/12] Training Accuracy1: 14.6438, Training Accuracy2: 12.0758, Loss1: 2.4669, Loss2: 2.3198, Animal10N dataset without pure ratio\n",
      "Train Epoch [2/200], Iter [200/12] Training Accuracy1: 29.7020, Training Accuracy2: 24.9160, Loss1: 2.0014, Loss2: 2.1907, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [3/200], Iter [100/12] Training Accuracy1: 18.1186, Training Accuracy2: 15.1993, Loss1: 2.0783, Loss2: 2.1486, Animal10N dataset without pure ratio\n",
      "Train Epoch [3/200], Iter [200/12] Training Accuracy1: 35.1241, Training Accuracy2: 31.8857, Loss1: 2.1452, Loss2: 2.1458, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [4/200], Iter [100/12] Training Accuracy1: 19.6061, Training Accuracy2: 19.0891, Loss1: 2.1903, Loss2: 2.1153, Animal10N dataset without pure ratio\n",
      "Train Epoch [4/200], Iter [200/12] Training Accuracy1: 40.1581, Training Accuracy2: 38.5747, Loss1: 1.9997, Loss2: 1.9741, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [5/200], Iter [100/12] Training Accuracy1: 20.8035, Training Accuracy2: 19.2212, Loss1: 1.9646, Loss2: 1.9924, Animal10N dataset without pure ratio\n",
      "Train Epoch [5/200], Iter [200/12] Training Accuracy1: 40.6493, Training Accuracy2: 39.2936, Loss1: 2.2231, Loss2: 2.1929, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-04-03 23:05:12,229]\u001B[0m Trial 182 pruned. \u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/200] Test Accuracy on the 5000 test images: Model1 23.6000 % Model2 23.5600 %, animal10n without pure ratio.\n",
      "Start trail...\n",
      "building model...\n",
      "Loading pre-trained model: EfficientNet_v2_m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Wenda/.cache\\torch\\hub\\rwightman_pytorch-image-models_master\n",
      "Using cache found in C:\\Users\\Wenda/.cache\\torch\\hub\\rwightman_pytorch-image-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parallel with 2 GPUs!\n",
      "Using parallel with 2 GPUs!\n",
      "Current model: EfficientNet_v2_m, current optimizer: SGD, criterion: CrossEntropyLoss(), lr schedule: StepLR, forget rate: 0.2, gradual rate: 10, mixup alpha: 0.05\n",
      "Training... animal10n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [1/200], Iter [100/12] Training Accuracy1: 40.5884, Training Accuracy2: 39.8117, Loss1: 1.8347, Loss2: 1.9248, Animal10N dataset without pure ratio\n",
      "Train Epoch [1/200], Iter [200/12] Training Accuracy1: 92.4966, Training Accuracy2: 91.2651, Loss1: 1.2549, Loss2: 1.3534, Animal10N dataset without pure ratio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenda\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... animal10n\n",
      "Train Epoch [2/200], Iter [100/12] Training Accuracy1: 59.6940, Training Accuracy2: 59.2540, Loss1: 0.8740, Loss2: 0.8954, Animal10N dataset without pure ratio\n",
      "Train Epoch [2/200], Iter [200/12] Training Accuracy1: 119.9308, Training Accuracy2: 119.8707, Loss1: 1.4073, Loss2: 1.3247, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [3/200], Iter [100/12] Training Accuracy1: 64.6935, Training Accuracy2: 64.3565, Loss1: 0.7388, Loss2: 0.7902, Animal10N dataset without pure ratio\n",
      "Train Epoch [3/200], Iter [200/12] Training Accuracy1: 129.4757, Training Accuracy2: 128.2796, Loss1: 0.9433, Loss2: 0.9884, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [4/200], Iter [100/12] Training Accuracy1: 67.0188, Training Accuracy2: 66.9796, Loss1: 0.8171, Loss2: 0.7134, Animal10N dataset without pure ratio\n",
      "Train Epoch [4/200], Iter [200/12] Training Accuracy1: 133.6363, Training Accuracy2: 133.5098, Loss1: 0.7238, Loss2: 0.6952, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [5/200], Iter [100/12] Training Accuracy1: 69.8240, Training Accuracy2: 69.1658, Loss1: 0.4802, Loss2: 0.5156, Animal10N dataset without pure ratio\n",
      "Train Epoch [5/200], Iter [200/12] Training Accuracy1: 141.5249, Training Accuracy2: 141.1374, Loss1: 0.4051, Loss2: 0.5478, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [5/200] Test Accuracy on the 5000 test images: Model1 75.4800 % Model2 75.5000 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [6/200], Iter [100/12] Training Accuracy1: 71.7974, Training Accuracy2: 71.3368, Loss1: 0.4999, Loss2: 0.4957, Animal10N dataset without pure ratio\n",
      "Train Epoch [6/200], Iter [200/12] Training Accuracy1: 142.8776, Training Accuracy2: 142.0642, Loss1: 0.4998, Loss2: 0.5206, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [7/200], Iter [100/12] Training Accuracy1: 72.8789, Training Accuracy2: 72.8539, Loss1: 1.9383, Loss2: 1.9299, Animal10N dataset without pure ratio\n",
      "Train Epoch [7/200], Iter [200/12] Training Accuracy1: 145.8688, Training Accuracy2: 145.0146, Loss1: 0.3364, Loss2: 0.3127, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [8/200], Iter [100/12] Training Accuracy1: 74.8506, Training Accuracy2: 74.9685, Loss1: 0.3453, Loss2: 0.4121, Animal10N dataset without pure ratio\n",
      "Train Epoch [8/200], Iter [200/12] Training Accuracy1: 149.0497, Training Accuracy2: 148.7970, Loss1: 0.5592, Loss2: 0.4852, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [9/200], Iter [100/12] Training Accuracy1: 75.1063, Training Accuracy2: 74.6246, Loss1: 0.2372, Loss2: 0.1902, Animal10N dataset without pure ratio\n",
      "Train Epoch [9/200], Iter [200/12] Training Accuracy1: 148.7577, Training Accuracy2: 147.6759, Loss1: 0.2503, Loss2: 0.2106, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [10/200], Iter [100/12] Training Accuracy1: 73.3927, Training Accuracy2: 73.4751, Loss1: 1.4262, Loss2: 1.3837, Animal10N dataset without pure ratio\n",
      "Train Epoch [10/200], Iter [200/12] Training Accuracy1: 147.8328, Training Accuracy2: 147.7431, Loss1: 0.2433, Loss2: 0.2642, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [10/200] Test Accuracy on the 5000 test images: Model1 78.5200 % Model2 78.5200 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [11/200], Iter [100/12] Training Accuracy1: 76.7664, Training Accuracy2: 76.9979, Loss1: 0.1880, Loss2: 0.1837, Animal10N dataset without pure ratio\n",
      "Train Epoch [11/200], Iter [200/12] Training Accuracy1: 151.9995, Training Accuracy2: 152.1716, Loss1: 0.3375, Loss2: 0.2563, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [12/200], Iter [100/12] Training Accuracy1: 77.0142, Training Accuracy2: 76.5443, Loss1: 0.1400, Loss2: 0.1226, Animal10N dataset without pure ratio\n",
      "Train Epoch [12/200], Iter [200/12] Training Accuracy1: 153.4822, Training Accuracy2: 153.5353, Loss1: 0.1404, Loss2: 0.1183, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [13/200], Iter [100/12] Training Accuracy1: 77.2529, Training Accuracy2: 77.3603, Loss1: 0.1013, Loss2: 0.0810, Animal10N dataset without pure ratio\n",
      "Train Epoch [13/200], Iter [200/12] Training Accuracy1: 155.3061, Training Accuracy2: 155.5196, Loss1: 0.1658, Loss2: 0.1802, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [14/200], Iter [100/12] Training Accuracy1: 77.4287, Training Accuracy2: 77.5074, Loss1: 0.2162, Loss2: 0.2517, Animal10N dataset without pure ratio\n",
      "Train Epoch [14/200], Iter [200/12] Training Accuracy1: 152.4771, Training Accuracy2: 152.0267, Loss1: 0.0687, Loss2: 0.1030, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [15/200], Iter [100/12] Training Accuracy1: 77.4678, Training Accuracy2: 77.3460, Loss1: 0.0740, Loss2: 0.1137, Animal10N dataset without pure ratio\n",
      "Train Epoch [15/200], Iter [200/12] Training Accuracy1: 156.4630, Training Accuracy2: 155.7177, Loss1: 1.4947, Loss2: 1.5939, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [15/200] Test Accuracy on the 5000 test images: Model1 79.9600 % Model2 79.5400 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [16/200], Iter [100/12] Training Accuracy1: 77.1621, Training Accuracy2: 78.0197, Loss1: 0.1785, Loss2: 0.2323, Animal10N dataset without pure ratio\n",
      "Train Epoch [16/200], Iter [200/12] Training Accuracy1: 154.7758, Training Accuracy2: 155.6936, Loss1: 0.2336, Loss2: 0.2523, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [17/200], Iter [100/12] Training Accuracy1: 77.5514, Training Accuracy2: 77.5888, Loss1: 0.1281, Loss2: 0.1145, Animal10N dataset without pure ratio\n",
      "Train Epoch [17/200], Iter [200/12] Training Accuracy1: 155.3810, Training Accuracy2: 155.3045, Loss1: 0.2824, Loss2: 0.2817, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [18/200], Iter [100/12] Training Accuracy1: 78.9076, Training Accuracy2: 78.7671, Loss1: 0.1075, Loss2: 0.1370, Animal10N dataset without pure ratio\n",
      "Train Epoch [18/200], Iter [200/12] Training Accuracy1: 157.3560, Training Accuracy2: 157.3249, Loss1: 0.1148, Loss2: 0.0875, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [19/200], Iter [100/12] Training Accuracy1: 78.1907, Training Accuracy2: 78.0905, Loss1: 0.2806, Loss2: 0.2374, Animal10N dataset without pure ratio\n",
      "Train Epoch [19/200], Iter [200/12] Training Accuracy1: 157.6090, Training Accuracy2: 157.5923, Loss1: 1.8921, Loss2: 1.7852, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [20/200], Iter [100/12] Training Accuracy1: 79.8429, Training Accuracy2: 80.0519, Loss1: 0.3011, Loss2: 0.2138, Animal10N dataset without pure ratio\n",
      "Train Epoch [20/200], Iter [200/12] Training Accuracy1: 158.0679, Training Accuracy2: 158.5260, Loss1: 0.1246, Loss2: 0.1045, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [20/200] Test Accuracy on the 5000 test images: Model1 80.7600 % Model2 80.1200 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [21/200], Iter [100/12] Training Accuracy1: 81.7549, Training Accuracy2: 81.4235, Loss1: 0.1472, Loss2: 0.1296, Animal10N dataset without pure ratio\n",
      "Train Epoch [21/200], Iter [200/12] Training Accuracy1: 160.4880, Training Accuracy2: 160.1989, Loss1: 0.6825, Loss2: 0.6470, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [22/200], Iter [100/12] Training Accuracy1: 79.6236, Training Accuracy2: 79.4281, Loss1: 0.1112, Loss2: 0.2004, Animal10N dataset without pure ratio\n",
      "Train Epoch [22/200], Iter [200/12] Training Accuracy1: 160.4407, Training Accuracy2: 159.8432, Loss1: 0.2802, Loss2: 0.2219, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [23/200], Iter [100/12] Training Accuracy1: 81.7114, Training Accuracy2: 81.5717, Loss1: 0.0355, Loss2: 0.0311, Animal10N dataset without pure ratio\n",
      "Train Epoch [23/200], Iter [200/12] Training Accuracy1: 160.4246, Training Accuracy2: 160.0393, Loss1: 1.1985, Loss2: 1.1580, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [24/200], Iter [100/12] Training Accuracy1: 80.0317, Training Accuracy2: 79.8550, Loss1: 0.0915, Loss2: 0.0532, Animal10N dataset without pure ratio\n",
      "Train Epoch [24/200], Iter [200/12] Training Accuracy1: 159.8107, Training Accuracy2: 159.6659, Loss1: 0.1546, Loss2: 0.0391, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [25/200], Iter [100/12] Training Accuracy1: 80.1828, Training Accuracy2: 79.9742, Loss1: 0.0463, Loss2: 0.0275, Animal10N dataset without pure ratio\n",
      "Train Epoch [25/200], Iter [200/12] Training Accuracy1: 160.8909, Training Accuracy2: 160.7961, Loss1: 0.7880, Loss2: 0.7803, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [25/200] Test Accuracy on the 5000 test images: Model1 81.2600 % Model2 81.2400 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [26/200], Iter [100/12] Training Accuracy1: 80.3793, Training Accuracy2: 80.0019, Loss1: 0.0817, Loss2: 0.0673, Animal10N dataset without pure ratio\n",
      "Train Epoch [26/200], Iter [200/12] Training Accuracy1: 161.3957, Training Accuracy2: 161.2851, Loss1: 0.0732, Loss2: 0.0696, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [27/200], Iter [100/12] Training Accuracy1: 79.6544, Training Accuracy2: 79.8267, Loss1: 0.3395, Loss2: 0.2423, Animal10N dataset without pure ratio\n",
      "Train Epoch [27/200], Iter [200/12] Training Accuracy1: 160.2186, Training Accuracy2: 159.6879, Loss1: 0.0916, Loss2: 0.1798, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [28/200], Iter [100/12] Training Accuracy1: 81.6418, Training Accuracy2: 81.8858, Loss1: 0.0965, Loss2: 0.0944, Animal10N dataset without pure ratio\n",
      "Train Epoch [28/200], Iter [200/12] Training Accuracy1: 163.0695, Training Accuracy2: 163.2939, Loss1: 1.4750, Loss2: 1.4872, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [29/200], Iter [100/12] Training Accuracy1: 80.6338, Training Accuracy2: 80.2570, Loss1: 0.0946, Loss2: 0.1191, Animal10N dataset without pure ratio\n",
      "Train Epoch [29/200], Iter [200/12] Training Accuracy1: 162.0151, Training Accuracy2: 161.7029, Loss1: 0.1513, Loss2: 0.0632, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [30/200], Iter [100/12] Training Accuracy1: 81.0316, Training Accuracy2: 80.5197, Loss1: 0.1637, Loss2: 0.1607, Animal10N dataset without pure ratio\n",
      "Train Epoch [30/200], Iter [200/12] Training Accuracy1: 163.0120, Training Accuracy2: 162.3070, Loss1: 0.0474, Loss2: 0.0249, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [30/200] Test Accuracy on the 5000 test images: Model1 81.4800 % Model2 81.4200 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n",
      "Train Epoch [31/200], Iter [100/12] Training Accuracy1: 81.0815, Training Accuracy2: 81.3736, Loss1: 0.1337, Loss2: 0.0880, Animal10N dataset without pure ratio\n",
      "Train Epoch [31/200], Iter [200/12] Training Accuracy1: 163.5776, Training Accuracy2: 163.9071, Loss1: 0.0911, Loss2: 0.0305, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [32/200], Iter [100/12] Training Accuracy1: 81.9965, Training Accuracy2: 82.4239, Loss1: 0.0262, Loss2: 0.0327, Animal10N dataset without pure ratio\n",
      "Train Epoch [32/200], Iter [200/12] Training Accuracy1: 164.3680, Training Accuracy2: 164.4000, Loss1: 0.0595, Loss2: 0.0882, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [33/200], Iter [100/12] Training Accuracy1: 82.9958, Training Accuracy2: 83.2021, Loss1: 0.1080, Loss2: 0.0625, Animal10N dataset without pure ratio\n",
      "Train Epoch [33/200], Iter [200/12] Training Accuracy1: 164.5962, Training Accuracy2: 164.9681, Loss1: 0.0287, Loss2: 0.0156, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [34/200], Iter [100/12] Training Accuracy1: 83.4786, Training Accuracy2: 83.6481, Loss1: 0.1508, Loss2: 0.1214, Animal10N dataset without pure ratio\n",
      "Train Epoch [34/200], Iter [200/12] Training Accuracy1: 166.8877, Training Accuracy2: 166.6523, Loss1: 0.0529, Loss2: 0.0745, Animal10N dataset without pure ratio\n",
      "Training... animal10n\n",
      "Train Epoch [35/200], Iter [100/12] Training Accuracy1: 80.6910, Training Accuracy2: 80.9953, Loss1: 1.5249, Loss2: 1.5945, Animal10N dataset without pure ratio\n",
      "Train Epoch [35/200], Iter [200/12] Training Accuracy1: 164.2693, Training Accuracy2: 164.1597, Loss1: 0.0222, Loss2: 0.0290, Animal10N dataset without pure ratio\n",
      "Evaluating animal10n...\n",
      "Start evaluating model 1\n",
      "Start evaluating model 2\n",
      "Epoch [35/200] Test Accuracy on the 5000 test images: Model1 81.9600 % Model2 82.0200 %, animal10n without pure ratio.\n",
      "Trial not pruned yet\n",
      "Training... animal10n\n"
     ]
    }
   ],
   "source": [
    "study_trial = True\n",
    "\n",
    "if name_dataset == \"cifar10\":\n",
    "    study_name = f\"{name_dataset}_Task{id_task}_2022_03_17\"  # update the date manually here\n",
    "else:  # When the dataset is animal10n\n",
    "    study_name = f\"{name_dataset}_unfreeze_weight_latest_additional_models\"\n",
    "\n",
    "# search_space = {\"Forget Rate\":[0.05, 0.1, 0.2, 0.5],\n",
    "#                 \"Gradual\":[10, 50],\n",
    "#                 \"Mixup Alpha\":[0, 0.05, 0.1, 0.5],\n",
    "#                 \"Optimizer\":[\"SGD\", \"Adam\"],\n",
    "#                 \"Scheduler\":[\"StepLR\", \"None\"],\n",
    "#                 \"Learning Rate\":[0.08, 0.04, 0.02, 0.008, 0.004],\n",
    "#                 \"Model\":[\"CNN\",\"Densenet121\",\"EfficientNet_v2_rw_t\"]\n",
    "#                 }\n",
    "# search_space = {\"Forget Rate\":[0.05, 0.1, 0.2, 0.5],\n",
    "#                 \"Gradual\":[10, 50],\n",
    "#                 \"Mixup Alpha\":[0, 0.05, 0.1, 0.5],\n",
    "#                 \"Optimizer\":[\"SGD\"],\n",
    "#                 \"Scheduler\":[\"StepLR\"],\n",
    "#                 \"Learning Rate\":[0.08, 0.04, 0.02, 0.008, 0.004],\n",
    "#                 \"Model\":[\"CNN\",\"Densenet121\",\"EfficientNet_v2_rw_t\"]\n",
    "#                 }\n",
    "\n",
    "\n",
    "if study_trial:\n",
    "    search_space = {\"Forget Rate\":[0.05, 0.1, 0.2, 0.5],\n",
    "                    \"Gradual\":[10, 50],\n",
    "                    \"Mixup Alpha\":[0, 0.05, 0.1, 0.5],\n",
    "                    \"Optimizer\":[\"SGD\"],\n",
    "                    \"Scheduler\":[\"StepLR\"],\n",
    "                    \"Learning Rate\":[0.08, 0.04, 0.02, 0.008, 0.004],\n",
    "                    \"Model\":[\"EfficientNet_v2_s\", \"EfficientNet_v2_m\", \"Mobilenet_v2_035\", \"Mobilenet_v2_100\"]\n",
    "                    }\n",
    "\n",
    "    # TODO: Configure MySQL DB for remote connection\n",
    "    study = optuna.create_study(study_name=study_name,\n",
    "                                storage='mysql://root:1115@192.168.50.11:3306/co_mixing',\n",
    "                                load_if_exists=True,\n",
    "                                direction=\"maximize\",\n",
    "                                sampler=optuna.samplers.GridSampler(search_space))\n",
    "\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Facts:\")\n",
    "    print(\"  Finished Trials: \", len(study.trials))\n",
    "    print(\"  Pruned Trials: \", len(pruned_trials))\n",
    "    print(\"  Complete Trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best Trial:\")\n",
    "    trial_best = study.best_trial\n",
    "    print(\"  Value: \", trial_best.value)\n",
    "    print(\"  Parameters: \")\n",
    "    for key, value in trial_best.params.items():\n",
    "        print(\"  {}: {}\".format(key, value))\n",
    "else:\n",
    "    train_trial(\"EfficientNet_v2_rw_t\",\n",
    "                \"SGD\",\n",
    "                \"StepLR\",\n",
    "                trial=None,\n",
    "                _forget_rate=0.5, _gradual=50, _num_epoch=200, _num_mixup_alpha=0.05, _iter_per_epoch=200,\n",
    "                _num_exponent=1, _learning_rate=0.02, _freq_evaluate=5, _freeze_weights=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# modelA = torch.hub.load('rwightman/pytorch-image-models', 'tf_efficientnetv2_s_in21ft1k')\n",
    "# modelB = torch.hub.load('rwightman/pytorch-image-models', 'tf_efficientnetv2_m_in21ft1k')\n",
    "# modelA = modelA.to(device)\n",
    "# modelB = modelB.to(device)\n",
    "#\n",
    "# _acc1, _acc2 = evaluate(test_loader, modelA, modelB)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.hub.list('rwightman/pytorch-image-models')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# import torch\n",
    "#\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# torch.cuda.empty_cache()\n",
    "# modelC = torch.hub.load('rwightman/pytorch-image-models', 'mobilenetv2_100', pretrained=True)\n",
    "# summary(modelC.to(device), input_size=(3, 32, 32))\n",
    "# modelC\n",
    "\n",
    "\n",
    "\n",
    "# from model import ReshapedPreTrainedModel, modify_pretrained_outputs\n",
    "# modelC = ReshapedPreTrainedModel(modelC, n_outputs=10, dropout_rate=0.25, freeze_weights=True)\n",
    "# modelC.to(device)\n",
    "# modelC = modify_pretrained_outputs(modelC)\n",
    "# modelC\n",
    "# modelC = modify_pretrained_outputs(modelC)\n",
    "# modelC.to(device)\n",
    "#\n",
    "# image, label = next(iter(test_loader))\n",
    "# image = image.to(device)\n",
    "# out = modelC(image)\n",
    "#\n",
    "# print(out.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# study = optuna.load_study(study_name=\"cifar10_1\",\n",
    "#                           storage='mysql://root:1115@192.168.50.11:3306/co_mixing')\n",
    "# print(study.best_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# optuna.delete_study(study_name=\"cifar10_Task1_2022_03_17\",\n",
    "                    # storage='mysql://root:1115@192.168.50.11:3306/co_mixing')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from optuna.visualization import plot_param_importances\n",
    "#\n",
    "# plot_param_importances(study=study)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}